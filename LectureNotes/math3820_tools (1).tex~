\chapter{Descartes' rule of signs}
\label{sec:descartes}
Descartes' rule of signs is a very useful way to study the existence and sign of the roots of a polynomial without having to express them explicitely.
\begin{atheorem}[Descartes' rule of signs]\label{th:descartes}
Let $p(x) = \sum_{i=0}^m a_ix^i $ be a polynomial with real coefficients such that $a_m \neq 0$.
Define $v$ to be the number of {\it variations in sign} of the sequence of coefficients $a_m, \ldots, a_0$. By `variations in sign' we mean the number of values of $n$ such that the sign of $a_n$ differs from the sign of $a_{n - 1}$, as $n$ ranges from $m$ down to 1.
Then 
\begin{itemize}
\item the number of positive real roots of $p(x)$ is $v-2N$ for some integer $N$ satisfying $0 \leq N \leq \dfrac{v}{2}$,
\item the number of negative roots of $p(x)$ may be obtained by the same method by applying the rule of signs to $p(-x)$.
\end{itemize}
\end{atheorem}


\begin{example}
Let $p(x) = x^3+3x^2-x-3$. The coefficients have sign $++--$, so there is one sign change.
Thus $v = 1$. Since $0 \leq N \leq 1/2$, we must have $N=0$. Thus $v-2N=1$ and there is exactly one positive real root of $p(x)$.

To find the negative roots, we examine $p(-x) = -x^3+3x^2+x-3$. The coefficients have sign $-++-$, so there are two sign changes. Thus $v=2$ and $0 \leq N \leq 2/2= 1$.
Thus, there are two possible solutions, $N=0$ and $N=1$, and two possible values of $v-2N$. Therefore, there are either two or no negative real roots.
Furthermore, note that $p(-1)=(-1)^3+3 \cdot (-1)^2-(-1)-3=0$, hence there is at least one negative root. Therefore there must be exactly two.
\end{example}

% \frame{\frametitle{Descartes' rule of signs}
% \begin{theorem}[Descartes' rule of signs]\label{th:descartes}
% Let $p(x) = \sum_{i=0}^m a_ix^i $ be a polynomial with real coefficients such that $a_m \neq 0$.
% Define $v$ to be the number of {\it variations in sign} of the sequence of coefficients $a_m, \ldots, a_0$. By 'variations in sign' we mean the number of values of $n$ such that the sign of $a_n$ differs from the sign of $a_{n - 1}$, as $n$ ranges from $m$ down to 1.
% Then 
% \begin{itemize}
% \item the number of positive real roots of $p(x)$ is $v-2N$ for some integer $N$ satisfying $0 \leq N \leq \dfrac{v}{2}$,
% \item the number of negative roots of $p(x)$ may be obtained by the same method by applying the rule of signs to $p(-x)$.
% \end{itemize}
% \end{theorem}
% }
% 
% \frame{\frametitle{Example of use of Descartes' rule}
% \begin{example}
% Let 
% \[
% p(x) = x^3+3x^2-x-3.
% \] 
% Coefficients have signs $++--$, i.e., 1 sign change.
% Thus $v = 1$. Since $0 \leq N \leq 1/2$, we must have $N=0$. Thus $v-2N=1$ and there is exactly one positive real root of $p(x)$.
% 
% To find the negative roots, we examine $p(-x) = -x^3+3x^2+x-3$. Coefficients have signs $-++-$, i.e., 2 sign changes. Thus $v=2$ and $0 \leq N \leq 2/2= 1$.
% Thus, there are two possible solutions, $N=0$ and $N=1$, and two possible values of $v-2N$. Therefore, there are either two or no negative real roots.
% Furthermore, note that $p(-1)=(-1)^3+3 \cdot (-1)^2-(-1)-3=0$, hence there is at least one negative root. Therefore there must be exactly two.
% \end{example}
% }


\chapter{Some matrix theory}

\section{Eigenvalues and eigenvectors}
\label{app:spectral}
Let $M\in\M_n(\IF)$ with $\IF=\IR$ or $\IC$. The \textbf{eigenvalues} of $M$ are numbers $\lambda\in\IC$ found by solving the equation
\begin{equation}\label{eq:eigenvalue_eq}
\det(M-\lambda\II)=0,
\end{equation}
where $\II$ is the identity matrix of $\M_n(\IF)$, and $v\in\IF^n$. Another way to write \eqref{eq:eigenvalue_eq} is as
\[
Mv=\lambda v.
\]
It is easy to see that these two expressions are equivalent. If $M\in\M_n(\IR)$, then there are exactly $n$ eigenvalues in $\IC$ (or $\IR$), including multiplicity. This set of values is called the \textbf{spectrum} of $M$, and is usually denoted $\Sp(M)$ or $\textrm{spec}(M)$. In other words,
\[
\Sp(M)=\{\lambda\in\IC:\det(M-\lambda\II)=0\textrm{ for some }v\in\IC^n\}.
\]
Note that eigenvalues are \emph{matrix invariants}, in the sense that they are preserved by linear transformations of the vector space. (Other examples of matrix invariants include the rank, the determinant and the trace.)
Another name for \eqref{eq:eigenvalue_eq} is the \textbf{characteristic polynomial}, which is obtained by considering the polynomial resulting from \eqref{eq:eigenvalue_eq},
\[
P(\lambda)=\det(M-\lambda\II).
\]
Eigenvalues of $M$ are then the roots of $P(\lambda)$.

To a given eigenvalue $\lambda_i\in\Sp(M)$, there corresponds an \textbf{eigenvector} $v_i$ which satisfies the equation \eqref{eq:eigenvalue_eq} for $\lambda=\lambda_i$.


\subsection{Left eigenvectors}
Let $M$ be an $r\times r$ matrix, $u,v$ be two column vectors, $\lambda\in\IR$. Then, if  
\[
Mu=\lambda u,
\]
$u$ is the (right) eigenvector corresponding to $\lambda$, and if
\[
v^TM=\lambda v^T
\]
then $v$ is the left eigenvector corresponding to $\lambda$. Note that to a given eigenvalue there corresponds (to a multiple) one left and one right eigenvector.


\section{Tools to determine properties of eigenvalues}
\begin{atheorem}(Routh-Hurwitz Criteria)
Given the polynomial,
$$P(\lambda)=\lambda ^n + a_1 \lambda ^{n-1}+\dots + a_{n-1}\lambda +a_n$$
where the coefficients $a_i$ are real constants, $i=1,\dots , n$ define the $n$ Hurwitz matrices using the coefficients $a_i$ of the characteristic polynomial:
$$H_1=(a_1),\quad H_2=\left (\begin{array}{cc}a_1 & 1 \\ a_3 &a_2\end{array}\right), \quad H_3=\left (\begin{array}{ccc}a_1 & 1 & 0 \\ a_3 &a_2 &a_1 \\ a_5& a_4 & a_3\end{array}\right),$$
and
$$H_n=\left (\begin{array}{cccccc}a_1 & 1 & 0 & 0 & \dots &0\\ a_3& a_2 & a_1 & 1 & \dots & 0 \\ a_5 & a_4 & a_3 & a_2 & \dots &0\\\vdots & \vdots & \vdots & \vdots & \dots & \vdots \\ 0 & 0& 0& 0& \dots & a_n\end{array}\right )$$
where $a_j=0$ if $j>n$. All of the roots of the polynomial $P(\lambda)$ are negative or have negative real part if and only if the determinants of all Hurwitz matrices are positive:
$$\det H_i>0, \quad j=1,2,\dots, n.$$
\end{atheorem}



\begin{atheorem}(Corollary)
Routh-Hurwitz criteria for $n=2,3,4,5$
\begin{itemize}
\item $n=2:$ $a_1>0$ and $a_2>0$.
\item $n=3:$ $a_1>0,$ $a_3>0$ and $a_1a_2>a_3$.
\item $n=4:$ $a_1>0,$ $a_3>0,$ $a_4>0$  and $a_1a_2a_3>a_3^2+a_1^2a_4$.
\item $n=5:$ $a_i>0,$ $i=1,2,3,4,5,$ $a_1a_2a_3>a_3^2+a_1^2a_4$ and $(a_1a_4-a_5)(a_1a_2a_3-a_3^2-a_1^2a_4)>a_5(a_1a_2-a_3)^2+a_1a_5^2$
\end{itemize}
\end{atheorem}



\begin{atheorem}(Gerhgorin's Theorem)
Let $A$ be an $n\times n$ matrix. Let $D_i$ be the disk in the complex plane with the center at $a_{ii}$ and radius $r_i=\sum _{j=1,j\not =i}^n|a_{ij}|$. Then all eigenvalues of the matrix $A$ lie in the union of the disks $D_i$, $i=1,2,\dots, n$, $\cup_{i=1}^nD_i$. In particular, if $\lambda $ is an eigenvalue of $A$, then for some $i=1,2,\dots,n$
$$|\lambda -a_{ii}|\leq r_i.$$
\end{atheorem}

\begin{atheorem}(Corollary)
Let $A$ be an $n\times n$ matrix with real entries. If the diagonal elements of $A$ satisfy 
$$a_{ii}<-r_{i} \quad where \quad r_i=\sum _{j=1,j\not =i}^n|a_{ij}|$$
for $i=1,2,\dots,n$ then the eigenvalues of $A$ are negative or have have negative real part.
\end{atheorem}

\section{Nonnegative matrices}
\begin{definition}
A matrix $A$ whose entries are nonnegative is called a \definesame{nonnegative matrix}, denoted $A\geq 0$.
\end{definition}


\begin{definition}
A matrix $A$ whose entries are positive is called a \definesame{positive matrix}, denoted $A> 0$.
\end{definition}


\begin{definition}
A square $m\times m$  matrix $A=a_{ij}$ is \define{reducible}{reducible matrix} if the index set $1,2, \dots, m$ can be split into two nonempty complementary sets $S_1$ and $S_2$: $S_1=\{i_1,\dots , i_{\mu}\}$ and $S_2=\{k_1,\dots , k_{\varepsilon}\}$ where $m=\mu+\varepsilon$ such that
$$a_{i_\alpha k_{\beta}}=0 \quad (\alpha = 1,2,\dots , \mu; \beta = 1,2, \dots, \varepsilon).$$
Otherwise, the matrix $A$ is \define{irreducible}{irreducible matrix}.
\end{definition}


\begin{definition}
If there exits a directed path from node $i$ to $j$ for every node $i$ and $j$ in the digraph, then the digraph is said to be \define{strongly connected}{strongly connected graph}.
\end{definition}

\begin{theorem}
The digraph of matrix $A$ is strongly connected if and only if $A$ is irreducible.
\end{theorem}


\begin{theorem}[Frobenius theorem\index{Frobenius theorem}]
An irreducible, nonnegative matrix $A$ always has a positive eigenvalues $\lambda$ that is a simple root (multiplicity one) of the characteristic equation. The value of $\lambda$ is greater than or equal to the magnitude of all the other eigenvalues. To the eignevalue $\lambda$, there corresponds an eigenvector with positive coordinates.
\end{theorem}


\begin{theorem}[Perron theorem\index{Perron theorem}]
A positive matrix $A$ always has a real, positive eigenvalue $\lambda$ that is a simple root of the characteristic equation and exceeds the magnitude of all of the other eigenvalues
$$|\lambda _i|<|\lambda|,\quad \forall i.$$
To the eigenvalue $\lambda$ there corresponds an eigenvector with positive coordinates.
\end{theorem}
In other words, $\rho(A)$ is a positive real eigenvalue of $A$ with multiplicity 1.

\begin{definition}[Primitivity]
If an irreducible, nonnegative matrix $A$ has $h$ eigenvalues $\lambda _1, \lambda _2, \dots \lambda _h$ of maximum modulus ($|\lambda _1 |= |\lambda _i|, i=1,2,\dots ,h $), then $A$ is called \define{primitive}{primitive matrix} if $h=1$ and \define{imprimitive}{imprimitive matrix} if $h>1$. The value of $h$ is called the \definesame{index of imprimitivity}.
\end{definition}

The index of imprimitivity is the number of eigenvalues of matrix $A$ with maximum modulus (with magnitude equal to $\rho(A)$).

\begin{theorem}
A nonnegative matrix $A$ is primitive if and only if some power of $A$ is positive (i.e. $A^p>0$ for some integer $p\geq 1$).
\end{theorem}

\begin{theorem}\cite{BermanPlemmons1994}
A irreducible matrix is primitive if its trace if positive. 
\end{theorem}

The following theorem is one of the most important in the theory of matrices.
\begin{theorem}(Perron-Frobenius Theorem)
If $M$ is a nonnegative primitive matrix, then:
\begin{itemize}
\item $M$ has a positive eigenvalue $\lambda_1$ of maximum modulus.
\item $\lambda_1$ is a simple root of the characteristic polynomial.
\item for every other eigenvalue $\lambda _i$, $\lambda_1>\lambda_i$ (it is strictly dominant)
\item $$\min_{i}\sum_j m_{ij}\leq \lambda _1 \leq \max_{i}\sum_j m_{ij} $$
$$\min_{j}\sum_i m_{ij}\leq \lambda _1 \leq \max_{j}\sum_i m_{ij} $$
\item the row and column eigenvectors associated with $\lambda_1$ are strictly positive.
\item the sequence $M^t$ is asymptotically one-dimensional, its columns converge to the column eigenvector associated with $\lambda_1$; and its rows converges to the row eigenvector associated with $\lambda_1$.
\end{itemize}
\end{theorem}


\subsection{Suggested reading}
Berman and Plemmons \cite{BermanPlemmons1979,BermanPlemmons1994} provides a very nice description of the links between nonnegative matrices and graphs. Senetta.. 

