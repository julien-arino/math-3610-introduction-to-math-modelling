\chapter[Traffic flow]{Traffic flow\\[0.5cm] Linear cascades\\ Linear systems\\
Delay differential equations\\ Laplace transform}
\label{chap:traffic_flow}



\section{An ODE model of traffic flow}

We want to model a situation with $N$ cars on a straight road with no overtaking, and where a given driver adjusts (instantaneously) their speed on the speed the driver in front of them.
\subsection{Hypotheses}
\begin{itemize}
\item $N$ cars in total,
\item the road is the $x$-axis,
\item $x_n(t)$ is the position of the $n$th car at time $t$,
\item $v_n(t)\stackrel{\Delta}{=}x_n'(t)$ is the velocity of the $n$th car at time $t$,
\item all cars start with the same initial speed $v_0$ at time $t=0$.
\end{itemize}
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.8\textwidth]{../figs_06_traffic_flow/traffic_flow1}
\caption{The situation described by the model of traffic flow.}
\label{fig:traffic_flow_ODEmodel}
\end{center}
\end{figure}
To make computations easier, we express the velocity of cars in a reference frame
moving at the speed $u_0$. (Remark that here, speed and velocity are equal, since movement is 1-dimensional.)
Let
\[
u_n(t)=v_n(t)-u_0.
\]
Then $u_n(t)=0$ for $t\leq 0$, and $u_n$ is the speed of the $n$th car in the moving frame coordinates.


\subsection{Modeling driver behavior}
Assume that a driver adjusts their speed according to relative speed between their car and the car in front, and that this adjustment is a linear term, equal to $\lambda$ for all drivers. This implies the following.
\begin{itemize}
\item
For the first car, the evolution of speed remains to be determined.
\item 
For the second car,
\[
u_2'=\lambda(u_1-u_2).
\]
This means that if the speed of the second car, $u_2$, is larger than that of the first car, $u_1$, then the driver of the second car decreases their speed ($u_2'$ has a negative sign). Similarly, if the speed of the second car is smaller than that of the first car, then the term $\lambda(u_2-u_1)$ is positive, implying that $u_2'>0$, i.e., the second car speeds up.
\item
Similarly, the third car adjusts its speeds as a function of the car in front, i.e., car number 2, giving
\[
u_3'=\lambda(u_2-u_3).
\]
\item 
Thus, for $n=1,\ldots,N-1$,
\begin{equation}\label{eq:ode}
u_{n+1}'=\lambda(u_n-u_{n+1}).
\end{equation}
\end{itemize}

\subsection{Solving using linear cascades}
This can be solved using \emph{linear cascades}: if $u_1(t)$ is known, then
\[
u_2'=\lambda(u_1(t)-u_2)
\]
is a linear first-order nonhomogeneous equation. The solution (obtained using integrating factors or variation of constants) is
\[
u_2(t)=\lambda e^{-\lambda t}\int_0^t u_1(s)e^{\lambda s}ds.
\]
Then use this function $u_2(t)$ in $u_3'$ to get $u_3(t)$,
\begin{align*}
u_3(t) &= \lambda e^{-\lambda t}\int_0^t u_2(s)e^{\lambda s}ds.
\end{align*}
Thus
\begin{align*}
u_3(t) &= \lambda e^{-\lambda t}\int_0^t u_2(s)e^{\lambda s}ds \\
&= \lambda e^{-\lambda t}\int_0^t 
\left(\lambda e^{-\lambda s}\int_0^s u_1(q)e^{\lambda q}dq\right)ds \\
&= \lambda^3 e^{-\lambda t}\int_0^t e^{-\lambda s}\int_0^s u_1(q)e^{\lambda q}dqds.
\end{align*}
Continue the process to get the solution.

\subsection{An example with known first driver behavior}
Suppose that the driver of car 1 follows the function
\[
u_1(t)=\alpha\sin(\omega t),
\]
that is, $\omega$-periodic, 0 at $t=0$ (we want all cars to start with speed relative to the moving reference frame equal to 0), and with amplitude $\alpha$.
Then
\begin{align*}
u_2(t) &= \lambda\alpha e^{-\lambda t}\int_0^t \sin(\omega s)e^{\lambda s}ds \\
&= \lambda\alpha e^{-\lambda t}
\left(
\frac{\omega-\omega e^{\lambda t} \cos(\omega t)+\lambda e^{\lambda t} \sin(\omega t)}{\lambda^2+\omega^2}
\right) \\
&= \frac{\lambda\alpha}{\lambda^2+\omega^2}\left(\omega e^{-\lambda t}
+\lambda\sin(\omega t)-\omega\cos(\omega t)\right).
\end{align*}
When $t\to\infty$, the first term goes to 0 and we are left with a $\omega$-periodic
term.
Continuing the process,
\[
u_3(t) = \frac{\lambda^2\alpha}{\lambda^2+\omega^2} e^{-\lambda t}
\int_0^t \left(\omega e^{-\lambda s}
+\lambda\sin(\omega s)-\omega\cos(\omega s)\right)e^{\lambda s}ds,
\]
that is,
\begin{align*}
u_3(t) &= \frac{\lambda^2\alpha}{\lambda^2+\omega^2}e^{-\lambda t}\left(\omega t+\int_0^t \left(\lambda\sin(\omega s)-\omega\cos(\omega s)\right)e^{\lambda s}ds\right) \\
&= \frac{\lambda^2\alpha}{\lambda^2+\omega^2}\left(\omega t+\frac{2 \lambda \omega}{\lambda^2+\omega^2}\right)e^{-\lambda t}\\
&\quad -\frac{\lambda^2\alpha}{(\lambda^2+\omega^2)^2}\left(2 \lambda \omega \cos(\omega t)-\lambda^2 \sin(\omega t)+\omega^2 \sin(\omega t)\right).
\end{align*}
Once again, the terms in $e^{-\lambda t}$ vanishes for large $t$, and we are left with 3 $\omega$-periodic terms.




\section{Linear systems of ODE -- Brief theory}

\begin{definition}[Linear ODE]
A \emph{linear} ODE is a differential equation taking the form
\begin{equation}\label{sys:linear_general}
\frac{d}{dt}x=A(t)x+B(t),
\end{equation}
where $A(t)\in\mathcal{M}_n(\IR)$ with continuous entries, $B(t)\in\IR^n$ with real valued, continuous coefficients, and $x\in\IR^n$. The associated IVP takes the form 
\begin{equation}\label{sys:IVP_linear_general}
\begin{aligned}
\frac{d}{dt}x &= A(t)x+B(t) \\
x(t_0)&=x_0.
\end{aligned}
\end{equation}
\end{definition}

\paragraph{Types of systems:}
\begin{itemize}
\item $x'=A(t)x+B(t)$ is linear nonautonomous ($A(t)$ depends on $t$) nonhomogeneous (also called \emph{affine} system).
\item $x'=A(t)x$ is linear nonautonomous homogeneous.
\item $x'=Ax+B$, that is, $A(t)\equiv A$ and $B(t)\equiv B$, is linear autonomous nonhomogeneous (or affine autonomous).
\item $x'=Ax$ is linear autonomous homogeneous.
\end{itemize}
\begin{itemize}
\item If $A(t+T)=A(t)$ for some $T>0$ and all $t$, then linear periodic.
\end{itemize}



\begin{theorem}[Existence and Uniqueness]
Solutions to \eqref{sys:IVP_linear_general} exist and are unique on the whole interval over which $A$ and $B$ are continuous.
In particular, if $A,B$ are constant, then solutions exist on $\IR$.
\end{theorem}


\frame{\frametitle{Autonomous linear systems}
Consider the autonomous affine system
\begin{equation}\label{sys:affine_auton}
\frac{d}{dt}x=Ax+B,
\end{equation}
and the associated homogeneous autonomous system
\begin{equation}\label{sys:lin_auton}
\frac{d}{dt}x=Ax.
\end{equation}
}

\subsection{Exponential of a matrix}
\begin{definition}[Matrix exponential]
Let $A\in\M_n(\IK)$ with $\IK=\IR$ or $\IC$. The \emph{exponential} of $A$, denoted $e^{At}$, is a matrix in $\M_n(\IK)$, defined by
\[
e^{At}=\II +\sum_{k=1}^\infty \frac{t^k}{k!}A^k,
\]
where $\II$ is the identity matrix in $\M_n(\IK)$.
\end{definition}

\paragraph{Properties of the matrix exponential}
\begin{itemize}
\item $e^{At_1}e^{At_2}=e^{A(t_1+t_2)}$ for all $t_1,t_2\in\IR$.
\item $Ae^{At}=e^{At}A$ for all $t\in\IR$. [$A$ and $e^{At}$ commute]
\item $(e^{At})^{-1}=e^{-At}$ for all $t\in\IR$.
\end{itemize}
\begin{theorem}\label{th:solution_matrix_exponential}
The unique solution $\phi$ of \eqref{sys:lin_auton} with initial condition $\phi(t_0)=x_0$ is given by
\[
\phi(t)=e^{A(t-t_0)}x_0.
\]
\end{theorem}


\subsection{Computing the matrix exponential}
Let $P$ be a nonsingular matrix in $\M_n(\IR)$. We transform the IVP
\begin{equation}\label{IVP:lin_auton}
\begin{aligned}
\frac{d}{dt}x &= Ax\\
x(t_0)&=x_0,
\end{aligned}
\end{equation}
using the transformation $x=Py$ or $y=P^{-1}x$.
The dynamics of $y$ is
\begin{align*}
y' &= (P^{-1}x)' \\
&= P^{-1}x' \\
&= P^{-1}Ax \\
&= P^{-1}APy.
\end{align*}
The initial condition is $y_0=P^{-1}x_0$.
We have thus transformed IVP \eqref{IVP:lin_auton} into
\begin{equation}\label{IVP:lin_auton2}
\begin{aligned}
\frac{d}{dt}y &= P^{-1}APy\\
y(t_0)&=P^{-1}x_0.
\end{aligned}
\end{equation}
By Theorem~\ref{th:solution_matrix_exponential}, the solution of \eqref{IVP:lin_auton2} is given by
\[
\psi(t)=e^{P^{-1}AP(t-t_0)}P^{-1}x_0,
\]
and since $x=Py$, the solution to \eqref{IVP:lin_auton} is given by
\[
\phi(t)=Pe^{P^{-1}AP(t-t_0)}P^{-1}x_0.
\]
So everything depends on $P^{-1}AP$.



\subsection{Matrix exponential -- Diagonalizable case}
Before we begin, recall the following definitions and results (see for example \cite{Franklin2000}).
\begin{definition}
Let $A,B\in\M_n(\IK)$. $A$ and $B$ are \emph{similar} if there exists $P\in\M_n(\IK)$ such that $P^{-1}AP=B$.
\end{definition}
\begin{theorem}
A matrix $A\in\M_n(\IK)$ is similar to a diagonal matrix if and only if $A$ has $n$ linearly independent eigenvectors. In particular, if $A$ has distinct eigenvalues, then $A$ is similar to a diagonal matrix.
\end{theorem}
Assume $P$ nonsingular in $\M_n(\IR)$ such that
\[
P^{-1}AP=
\begin{pmatrix}
\lambda_1 & & 0 \\
& \ddots & \\
0 & & \lambda_n
\end{pmatrix}
\]
with all eigenvalues $\lambda_1,\ldots,\lambda_n$ different.
We have
\[
e^{P^{-1}AP}=\II+\sum_{k=1}^\infty\frac{t^k}{k!}
\begin{pmatrix}
\lambda_1 & & 0 \\
& \ddots & \\
0 & & \lambda_n
\end{pmatrix}^k
\]
For a (block) diagonal matrix $M$ of the form
\[
M=
\begin{pmatrix}
m_{11} & & 0\\
& \ddots & \\
0 & & m_{nn}
\end{pmatrix}
\]
there holds
\[
M^k=
\begin{pmatrix}
m_{11}^k & & 0\\
& \ddots & \\
0 & & m_{nn}^k
\end{pmatrix}
\]
Therefore,
\begin{align*}
e^{P^{-1}AP} &= \II+\sum_{k=1}^\infty\frac{t^k}{k!}
\begin{pmatrix}
\lambda_1^k & & 0 \\
& \ddots & \\
0 & & \lambda_n^k
\end{pmatrix} \\
&=\begin{pmatrix}
\sum_{k=0}^\infty\frac{t^k}{k!}\lambda_1^k & & 0 \\
& \ddots & \\
0 & & \sum_{k=0}^\infty\frac{t^k}{k!}\lambda_n^k
\end{pmatrix}\\
&=\begin{pmatrix}
e^{\lambda_1t} & & 0 \\
& \ddots & \\
0 & & e^{\lambda_nt}
\end{pmatrix}
\end{align*}
And so the solution to \eqref{IVP:lin_auton} is given, in the case that $A$ is diagonalizable, by
\begin{equation}\label{sol:linear_sys_ODE_diagonalizable}
\phi(t)=P
\begin{pmatrix}
e^{\lambda_1t} & & 0 \\
& \ddots & \\
0 & & e^{\lambda_nt}
\end{pmatrix}
P^{-1}x_0.
\end{equation}



\subsection{Matrix exponential -- Nondiagonalizable case}
\begin{definition}[Generalized eigenvectors]
Let $A\in\M_n(\IR)$. Suppose $\lambda$ is an eigenvalue of $A$ with multiplicity $m\leq n$. Then, for $k=1,\ldots,m$, any nonzero solution $v$ of
\[
(A-\lambda\II)^kv=0
\]
is called a \emph{generalized eigenvector} of $A$.
\end{definition}


\begin{definition}[Nilpotent matrix]
Let $A\in\M_n(\IR)$. $A$ is \emph{nilpotent} (of order $k$) if $A^{j}\neq 0$ for $j=1,\ldots,k-1$, and $A^k=0$.
\end{definition}


\begin{theorem}[Jordan normal form]
\label{th:Jordan_form}
Let $A\in\M_n(\IR)$ have eigenvalues $\lambda_1,\ldots,\lambda_n$, repeated according to their multiplicities. 
\begin{itemize}
\item 
Then there exists a basis of generalized eigenvectors for $\IR^n$. 
\item 
And if $\{v_1,\ldots,v_n\}$ is any basis of generalized eigenvectors for $\IR^n$, then the matrix $P=[v_1\cdots v_n]$ is invertible, and $A$ can be written as
\[
A=S+N,
\]
where
\[
P^{-1}SP=\diag(\lambda_j),
\]
the matrix $N=A-S$ is nilpotent of order $k\leq n$, and $S$ and $N$ commute, i.e., $SN=NS$.
\end{itemize}
\end{theorem}
Another formulation of the same theorem, which is ``self-contained'', is as follows.
\begin{theorem}
Let $A\in\M_n(\IR)$ with eigenvalues $\lambda_1,\ldots,\lambda_s$ with multiplicities $m_1,\ldots,m_s$:
\[
\det\left(A-\lambda I\right)=\prod_{j=1}^s(\lambda-\lambda_j)^{m_j}.
\]
Then $A$ is similar to a matrix of the form
\[
J=\diag\left(\Lambda_1,\ldots,\Lambda_s\right),
\]
where each block $\lambda_i$ is an $m_i\times m_i$-matrix of the form
\[
\Lambda_i=
\begin{pmatrix}
\lambda_i & \ast & 0 &\cdots & 0 \\
0 & \lambda_i & \ast & \cdots & 0 \\
0 & & & & 0 \\
0 & & & \lambda_i & \ast \\
& & & 0 & \lambda_i
\end{pmatrix}.
\]

\end{theorem}


The Jordan canonical form is
\[
P^{-1}AP=
\begin{pmatrix}
J_0 & & 0\\
& \ddots & \\
0 & & J_s
\end{pmatrix},
\]
so we use the same property as before (but with block matrices now), and
\[
e^{P^{-1}APt}=
\begin{pmatrix}
e^{J_0t} & & 0\\
& \ddots & \\
0 & & e^{J_st}
\end{pmatrix}.
\]
The first block in the Jordan canonical form takes the form
\[
J_0=
\begin{pmatrix}
\lambda_0 & & 0\\
& \ddots & \\
0 & & \lambda_k
\end{pmatrix}
\]
and thus, as before,
\[
e^{J_0t}=
\begin{pmatrix}
e^{\lambda_0t} & & 0\\
& \ddots & \\
0 & & e^{\lambda_kt}
\end{pmatrix}.
\]
Other blocks $J_i$ are written as
\[
J_i=\lambda_{k+i}\II+N_i,
\]
with $\II$ the $n_i\times n_i$ identity and $N_i$ the $n_i\times n_i$ nilpotent matrix
\[
N_i=\begin{pmatrix}
0 & 1 & 0 && 0 \\
&&\ddots && \\
&&&& 1 \\
0& &&&0
\end{pmatrix}.
\]
$\lambda_{k+i}\II$ and $N_i$ commute, and thus
\[
e^{J_it}=e^{\lambda_{k+i}t}e^{N_it}.
\]
Since $N_i$ is nilpotent, $N_i^k=0$ for all $k\geq n_i$. Therefore, the series $e^{N_it}$ terminates and
\[
e^{J_it}=e^{\lambda_{k+i}t}
\begin{pmatrix}
1 & t & \cdots & \frac{t^{n_i-1}}{(n_i-1)!} \\
0 & 1 & \cdots & \frac{t^{n_i-2}}{(n_i-2)!} \\
&&& \\
0 &&& 1
\end{pmatrix}.
\]


\begin{theorem}
For all $(t_0,x_0)\in\IR\times\IR^n$, there is a unique solution $x(t)$ to \eqref{IVP:lin_auton} defined for all $t\in\IR$. Each coordinate function of $x(t)$ is a linear combination of functions of the form
\[
t^ke^{\alpha t}\cos(\beta t)\quad\textrm{and}\quad t^ke^{\alpha t}\sin(\beta t)
\]
where $\alpha+i\beta$ is an eigenvalue of $A$ and $k$ is less than the algebraic multiplicity of the eigenvalue.
\end{theorem}
\begin{theorem}
Under conditions of the Jordan normal form Theorem~\ref{th:Jordan_form}, the linear system $x'=Ax$ with initial condition $x(0)=x_0$, has solution
\[
x(t)=P\diag\left(e^{\lambda_j t}\right)P^{-1}\left(\II+Nt+\cdots\frac{t^k}{k!}N^k\right) x_0.
\]
\end{theorem}
The result is particularly easy to apply in the following case, where we do not need the matrix $P$ (the basis of generalized eigenvectors).
\begin{theorem}[Case of an eigenvalue of multiplicity $n$]
\label{th:Jordan_simple_case}
Suppose that $\lambda$ is an eigenvalue of multiplicity $n$ of $A\in\M_n(\IR)$. Then $S=\diag(\lambda)$, and the solution of $x'=Ax$ with initial value $x_0$ is given by
\[
x(t)=e^{\lambda t}\left(\II+Nt+\cdots\frac{t^k}{k!}N^k\right) x_0.
\]
\end{theorem}


Finally, we give the following variation of constants formula, using the matrix exponential.
\begin{theorem}[Variation of constants formula]
Consider the IVP
\begin{subequations}\label{sys:lin_nonlin}
\begin{align}
x' &= Ax+B(t) \\
x(t_0) &= x_0,
\end{align}
\end{subequations}
where $B:\IR\to\IR^n$ a smooth function on $\IR$, and let $e^{A(t-t_0)}$ be matrix exponential associated to the homogeneous system $x'=Ax$. Then the solution $\phi$ of \eqref{sys:lin_nonlin} is given by
\begin{equation}
\phi(t)=e^{A(t-t_0)}x_0 + \int_{t_0}^t e^{A(t-s)}B(s)ds.
\end{equation}
\end{theorem}





\section{Linear systems -- Our case}
\subsection{General computations, case of 3 cars}
We consider the case of 3 cars. Let
\[
X=\begin{pmatrix}
u_2\\ u_3
\end{pmatrix}
\]
Then the system can be written as
\[
X'=\begin{pmatrix}
-\lambda & 0 \\
\lambda & -\lambda
\end{pmatrix}X
+\begin{pmatrix}
\lambda u_1(t)\\ 0
\end{pmatrix}
\]
which we write for short as $X'=AX+B(t)$.
The matrix $A$ has the eigenvalue $-\lambda$ with multiplicity 2. Its Jordan form can be obtained using the maple function {\tt JordanForm}:
\begin{verbatim}
> with(LinearAlgebra)
> A := <<-lambda, lambda> | <0, -lambda>>:
> J := JordanForm(A)
\end{verbatim}
giving
\[
J=\begin{pmatrix}
-\lambda & 1 \\
0 & -\lambda
\end{pmatrix}
\]
To get the matrix of change of basis, 
\begin{verbatim}
> P := JordanForm(A,output='Q')
\end{verbatim}
giving
\[
P=\begin{pmatrix}
0 & 1 \\
\lambda & 0
\end{pmatrix}
\]
which is such that $P^{-1}AP=J$.
Because $-\lambda$ is an eigenvalue with multiplicity 2 (the same multiplicity as as the size of the matrix), we can use the simplified theorem Theorem~\ref{th:Jordan_simple_case}, and only need $N$. 
We have
\begin{align*}
N &= A-S \\
&=\begin{pmatrix}
-\lambda & 0 \\
\lambda & -\lambda
\end{pmatrix}
-
\begin{pmatrix}
-\lambda & 0 \\
0 & -\lambda
\end{pmatrix} \\
&=\begin{pmatrix}
0 & 0 \\
\lambda & 0
\end{pmatrix}.
\end{align*}
Clearly, $N^2=0$, so, by the theorem in the simplified case,
\begin{align*}
x(t) &= e^{-\lambda t}\left(\II+Nt\right)x_0 \\
\end{align*}
But we know that solutions are unique, and that the solution to the differential equation is given by $x(t)=e^{At}x_0$. This means that
\begin{align*}
e^{At} &= e^{-\lambda t}\left(\II+Nt\right)\\
&= e^{-\lambda t}\left(\begin{pmatrix}1&0\\0&1\end{pmatrix}+
\begin{pmatrix}
0 & 0 \\
\lambda t & 0
\end{pmatrix}\right)\\
&= e^{-\lambda t}\begin{pmatrix}
1 & 0 \\
\lambda t & 1
\end{pmatrix} \\
&= \begin{pmatrix}
e^{-\lambda t} & 0 \\
\lambda te^{-\lambda t} & e^{-\lambda t}
\end{pmatrix}.
\end{align*}
Now notice that the solution to
\[
X'=AX
\]
is trivially established here, since
\[
X(0)=\begin{pmatrix}u_2(0)\\u_3(0)\end{pmatrix}=\begin{pmatrix}0\\0\end{pmatrix},
\]
and thus
\[
X(t)=e^{At}0=0.
\]
The matrix $e^{At}$ does however play a role in the solution (fortunately), since it is involved in the variation of constants formula:
\[
X(t)=e^{At}X_0+\int_0^t e^{A(t-s)}B(s)ds.
\]
Thus we need to compute $e^{A(t-s)}B(s)$, and then the integral.
\begin{align*}
e^{A(t-s)}B(s) &= 
\begin{pmatrix}
e^{-\lambda (t-s)} & 0 \\
\lambda (t-s)e^{-\lambda (t-s)} & e^{-\lambda (t-s)}
\end{pmatrix}
\begin{pmatrix}
\lambda u_1(s)\\ 0
\end{pmatrix} \\
&= \begin{pmatrix}
\lambda e^{-\lambda(t-s)}u_1(s) \\
\lambda^2e^{-\lambda(t-s)} (t-s)u_1(s)
\end{pmatrix}
\end{align*}
and thus
\begin{align*}
\int_0^t e^{A(t-s)}B(s)ds &=
\int_0^t\left(
\begin{array}{ll}
\lambda e^{-\lambda(t-s)}u_1(s) \\
\lambda^2e^{-\lambda(t-s)} (t-s)u_1(s)
\end{array}
\right)ds \\
&= \left(\begin{array}{ll}
\int_0^t\lambda e^{-\lambda(t-s)}u_1(s) ds \\
\int_0^t\lambda^2e^{-\lambda(t-s)} (t-s)u_1(s) ds
\end{array}
\right) \\
&= \left(\begin{array}{ll}
\lambda e^{-\lambda t} \int_0^te^{\lambda s}u_1(s) ds \\
\lambda^2 e^{-\lambda t}\int_0^te^{\lambda s} (t-s)u_1(s) ds
\end{array}
\right) \\
&= \left(\begin{array}{ll}
\lambda e^{-\lambda t} \int_0^te^{\lambda s}u_1(s) ds \\
\lambda^2 e^{-\lambda t}\left(t\int_0^te^{\lambda s}u_1(s) ds
-\int_0^tse^{\lambda s} u_1(s) ds\right)
\end{array}
\right).
\end{align*}
Let
\[
\Psi(t)=\int_0^t e^{\lambda s} u_1(s)ds
\]
and
\[
\Phi(t)=\int_0^t se^{\lambda s} u_1(s)ds.
\]
These can be computed when we choose a function $u_1(t)$. Then, finally, we have
\begin{align*}
X(t) &= \int_0^t e^{A(t-s)}B(s)ds \\
&= \left(\begin{array}{ll}
\lambda e^{-\lambda t} \Psi(t) \\
\lambda^2 e^{-\lambda t}\left(t\Psi(t)
-\Phi(t)\right)
\end{array}
\right).
\end{align*}


\subsection{Specialization to the case of the $\alpha\sin(\omega t)$ driver}
We set
\[
u_1(t)=\alpha\sin(\omega t).
\]
Then
\begin{align*}
\Psi(t) &= \frac{\alpha (\omega-\omega e^{\lambda t} \cos(\omega t)+\lambda e^{\lambda t} \sin(\omega t))}{\lambda^2+\omega^2}
\end{align*}
and
\begin{align*}
\Phi(t) &=\frac{\alpha(\lambda^3 t+\lambda t \omega^2-\lambda^2+\omega^2) \sin(\omega t) e^{\lambda t}}{(\lambda^2+\omega^2)^2}\\
&\quad -\frac{\alpha\omega \cos(\omega t) (t \lambda^2+t \omega^2-2 \lambda) e^{\lambda t}+2\alpha \lambda \omega}{(\lambda^2+\omega^2)^2}
\end{align*}


\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.49\textwidth]{../figs_06_traffic_flow/traffic_flow_3cars_ODE_l0}
\includegraphics[width=0.49\textwidth]{../figs_06_traffic_flow/traffic_flow_3cars_ODE_l04}
\end{center}
\caption{$\lambda=0$ (left)and $\lambda=0.4$ (right).}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.49\textwidth]{../figs_06_traffic_flow/traffic_flow_3cars_ODE_l08}
\includegraphics[width=0.49\textwidth]{../figs_06_traffic_flow/traffic_flow_3cars_ODE_l5}
\end{center}
\caption{$\lambda=0.8$ (left) and $\lambda=5$ (right).}
\end{figure}


\section{Traffic flow -- DDE model}
In the previous model, reaction time is instantaneous.
In practice, this is known to be incorrect: reflexes and psychology play a role.
It takes at least a few instants to acknowledge a change of speed in the car in front.
If the change of speed is not threatening, then you may not want to react right away.
When you press the accelerator or the brake, there is a delay between the action and the reaction..

We consider the same setting as previously, except that now, for $t>0$,
\begin{equation}\label{eq:dde}
u'_{n+1}(t)=\lambda(u_n(t-\tau)-u_{n+1}(t-\tau)),
\end{equation}
for $n=1,\ldots,N-1$. Here, $\tau\geq 0$ is called the \emph{time delay} (or \emph{time lag}), or for short, \emph{delay} (or \emph{lag}).
If $\tau=0$, we are back to the previous model.

\paragraph{Initial data}
For a delay equation such as \eqref{eq:dde}, the initial conditions become \emph{initial data}. This initial data must be specified on an interval of length $\tau$, left of zero.
\vskip0.5cm
This is easy to see by looking at the terms: $u(t-\tau)$ involves, at time $t$, the state of $u$ at time $t-\tau$. So if $t<\tau$, we need to know what happened for $t\in[-\tau,0]$.
So, normally, we specify initial data as
\[
u_n(t)=\phi(t)\textrm{ for }t\in[-\tau,0],
\]
where $\phi$ is some function, that we assume to be continuous. We assume $u_1(t)$ is known.
Here, we assume, for $n=1,\ldots,N$,
\begin{equation}\label{sys:IC_DDE_traffic_flow}
u_n(t)=0,\qquad t\leq (n-1)\tau.
\end{equation}
The explanation for this form of the initial data is simple. Since it takes $\tau$ units of time for each driver to adjust to changes in the speed of the car in front of them, car number 1 has to have moved for $\tau$ units of time before car number 2 makes any adjustment. In turn, car number 2 must have moved for $\tau$ units of time before car number 3 makes any adjustment. Repeating this, we obtain the form \eqref{sys:IC_DDE_traffic_flow}.

\frame{\frametitle{Important remark}
Although \eqref{eq:dde} looks very similar to \eqref{eq:ode}, you must keep in mind that it is in fact much more complicated.
\begin{itemize}
\item
A solution to \eqref{eq:ode} is a continuous function from $\IR$ to $\IR$ (or to $\IR^n$ if we consider the system).
\item
A solution to \eqref{eq:dde} is a continuous function in the space of continuous functions.
\item
The space $\IR^n$ has dimension $n$. The space of continuous functions has dimension $\infty$.
\end{itemize}
We can use the Laplace transform to get some understanding of the nature of the solutions.
}

\section{The Laplace transform}

\begin{definition}[Laplace transform]
Let $f(t)$ be a function defined for $t\geq 0$. The \emph{Laplace transform} of $f$ is the function $F(s)$ defined by
\[
F(s)=\L\{f(t)\}=\int_0^\infty e^{-st}f(t)dt.
\]
\end{definition}
The Laplace transform is a linear operator:
\[
\L\{af(t)+bg(t)\}=a\L\{f(t)\}+b\L\{g(t)\}.
\]


We have the following rules of transformation:
\begin{center}
\begin{tabular}{ll}
$t$-domain & $s$-domain\\
\hline
$af(t)+bg(t)$ & $aF(s)+bG(s)$ \\
$tf(t)$ & $-F'(s)$ \\
$t^nf(t)$ & $(-1)^nF^{(n)}(s)$ \\
$f'$ & $sF(s)-f(0)$ \\
$f''$ & $s^2F(s)-sf(0)-f'(0)$ \\
$f^{(n)}$ & $s^nF(s)-s^{n-1}f(0)-\cdots-f^{(n-1)}(0)$ \\
$\frac{f(t)}{t}$ & $\int_s^\infty F(u)du$ \\
$\int_0^t f(u)du=u(t)\ast f(t)$ & $\frac 1s F(s)$ \\
$f(at)$ & $\frac 1{|a|}F\left(\frac sa\right)$ \\
$e^{at}f(t)$ & $F(s-a)$ \\
$f(t-a)u(t-a)$ & $e^{-as}F(s)$ \\
$f(t)\ast g(t)$ & $F(s)G(s)$
\end{tabular}
\end{center}
Here $f^{(n)}$ represents the $n$th derivative, not the $n$th iterate, and $\ast$ is the convolution product,
\[
(f\ast g)(t)=\int_a^b f(s)g(t-s)ds.
\]


\frame{\frametitle{Dirac delta -- Heaviside function}
In the table on the following slide,
\begin{itemize}
\item $\delta(t)$ is the Dirac delta,
\[
\delta(t)=\begin{cases}
\infty & \textrm{if }t=0\\
0 & \textrm{if }t\neq 0.
\end{cases}
\]
\item $H(t)$ is the Heaviside function,
\[
H(t)=\begin{cases}
0 & \textrm{if }t< 0 \\
1 & \textrm{if }t> 0.
\end{cases}
\]
\end{itemize}
Note that $H(t)=\int_{-\infty}^t \delta(s)ds$.
}

\frame{\frametitle{Transforms of common functions}
\begin{center}
\begin{tabular}{ll}
$t$-domain & $s$-domain\\
\hline
$\delta(t)$ & $1$ \\
$\delta(t-\tau)$ & $e^{-\tau s}$ \\
$H(t)$ & $\frac 1s$ \\
$H(t-\tau)$ & $\frac{e^{-\tau s}}s$ \\
$\frac{t^n}{n!}H(t)$ & $\frac{1}{s^{n+1}}$ \\
$e^{-\alpha t}H(t)$ & $\frac 1{s+\alpha}$ \\
$\sin(\omega t)H(t)$ & $\frac{\omega}{s^2+\omega^2}$ \\
$\cos(\omega t)H(t)$ & $\frac s{s^2+\omega^2}$ \\
\end{tabular}
\end{center}
}

\begin{definition}
Given a function $F(s)$, if there exists $f(t)$, continuous on $[0,\infty)$ and such that
\[
\L\{f\}=F,
\]
then $f(t)$ is the \emph{inverse Laplace transform} of $F(s)$, and is denoted $f=\L^{-1}\{F\}$.
\end{definition}
\begin{theorem}
The inverse Laplace transform is a linear operator. Assume that $\L^{-1}\{F_1\}$ and $\L^{-1}\{F_2\}$ exist, then
\[
\L^{-1}\{aF_1+bF_2\}=a\L^{-1}\{F_1\}+b\L^{-1}\{F_2\}.
\]
\end{theorem}

\paragraph{Solving differential equations using the Laplace transform}
\begin{enumerate}
\item Take the Laplace transform of both sides of the equation.
\item Using the initial conditions, deduce an algebraic system of equations in $s$-space.
\item Solve the algebraic system in $s$-space.
\item Take the inverse Laplace transform of the solution in $s$-space, to obtain the solution of the differential equation in $t$-space.
\end{enumerate}




\section[Laplace of the DDE]{Laplace transform of our DDE traffic flow model}


\frame{
Let
\[
U_{k+1}(s)=\L\{u_{k+1}(t)\}=\int_0^\infty e^{-st}u_{k+1}(t)dt.
\]
Since we have assumed initial data of the form
\[
u_n(t)=0\qquad\textrm{for } t\leq(n-1)\tau,
\]
we have
\[
U_{k+1}(s)=\int_{k\tau}^\infty e^{-st}u_{k+1}(t)ds.
\]
}

\frame{
Since $u_{n+1}(t)=0$ for $t\leq n\tau$,
\begin{align*}
\int_0^\infty e^{-st}u_{n+1}'(t)dt &= \left[u_{k+1}(t)e^{-st}\right]_{k\tau}^\infty +s\int_{k\tau}^\infty e^{-st}u_{k+1}(t)dt \\
&= sU_{k+1}(s)
\end{align*}
and
\begin{align*}
\int_0^\infty e^{-st}u_{k+1}(t-\tau)dt &= \int_{(k-1)\tau}^\infty e^{-st}u_{k+1}(t-\tau)dt \\
&= \int_{(k-2)\tau}^\infty e^{-s(t+\tau)}u_k(\tau)d\tau \\
&= e^{-s\tau}U_k(s),
\end{align*}
since $e^{-st}u_{k+1}(t)\to 0$ for the improper integral to exist.

Note that we could have obtained this directly using the properties of the Laplace transform.
}

\frame{
Multiply 
\[
u_{n+1}'(t)=\lambda(u_{n}(t-\tau)-u_{n+1}(t-\tau))
\]
by $e^{-st}$,
\[
e^{-st}u_{n+1}'(t)=\lambda e^{-st}(u_{n}(t-\tau)-u_{n+1}(t-\tau))
\]
integrate over $(0,\infty)$ (using the expressions found above),
\[
sU_{n+1}(s)=\lambda(e^{-s\tau}U_{n}(s)-e^{-s\tau}U_{n+1}(s))
\]
which is equivalent to
\[
U_{n+1}(s)=\frac{\lambda U_n(s)}{\lambda+se^{s\tau}}
\]
Thus, when $U_1(s)$ is known, we can deduce the values for all $U_n$.
}

\frame{
Suppose 
\[
u_1(t)=\alpha \sin(\omega t)
\]
From the table of Laplace transforms, it follows that 
\[
U_1(s)=\alpha\frac{\omega}{s^2+\omega^2}
\]
Therefore,
\[
U_2=\frac{\lambda U_1(s)}{\lambda+se^{st}}
=\alpha\frac{\lambda}{\lambda+se^{st}}\;\frac{\omega}{s^2+\omega^2}
\]
and we can continue.. 
\vskip0.5cm
However, even though we know the solution in $s$-space, it is difficult to get the behavior in $t$-space, by hand, and maple does not help us either.
}