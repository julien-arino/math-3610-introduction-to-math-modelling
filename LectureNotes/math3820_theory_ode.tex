\chapter{A brief theory of ordinary differential equations}
\label{chap:theory_ode}


In this chapter, we consider ordinary differential equations of first-order,
\begin{equation}\label{eq:1st_order_ODE_nonauton}
x'=f(t,x),
\end{equation}
with $x\in\IR^n$ and $f:\IR\times\IR^n\to\IR^n$, as well as $p$th-order ordinary differential equations of the form
\begin{equation}\label{eq:pth_order_ODE}
x^{(p)}=f(t,x,x',\ldots,x^{(p-1)}),
\end{equation}
where $x\in\IR$ and $x^{(k)}$ denotes the $k$th derivative of $x(t)$. These two equations are said to be in \emph{normal form}, and form a subset of a larger class of first-order and $p$th-order differential equations, defined respectively by $F(t,x,x')=0$ and $F(t,x,x',\ldots,x^{(p)})=0$. We could also consider vector-valued $p$th-order equations.
However, \eqref{eq:1st_order_ODE} and \eqref{eq:pth_order_ODE} are sufficient for our purpose, and we limit our attention to them.

\section{First definitions}
A system such as \eqref{eq:1st_order_ODE} is \emph{nonautonomous}, since the function $f$ depends explicitly on $t$ (time). Much of the theory in this chapter will concern specifically the \emph{autonomous} system,
\begin{equation}\label{eq:1st_order_ODE}
x'=f(x),
\end{equation}
where $x\in\IR^n$ and $f:\IR^n\to\IR^n$.

\begin{definition}
An equilibrium solution of equation \eqref{eq:1st_order_ODE} is a solution $\bar x$ satisfying 
\[
f(\bar x)=0.
\]
\end{definition}
The system at $x=\bar x$ is then said to be at equilibrium. Indeed, since $f(\bar x)=0$, we have
\[
\frac{d}{dt}\bar x(t)=f(\bar x(t))=0,
\]
and thus the system is ``at rest''. Equilibria are then classified in terms of their \emph{stability}. We return to this concept later, but we give the definitions now because they appear throughout this chapter.
\begin{definition}(Local stability)
An equilibrium solution $\bar x$ of \eqref{eq:1st_order_ODE} is \emph{locally stable} if for each $\epsilon>0$ there exits a $\delta >0$ such that every solution $x(t)$ of \eqref{eq:1st_order_ODE} with the initial condition $x(t_0)=x_0$,
$$\| x_0-\bar x\|_2< \delta,$$
satisfies the condition that
$$\| x(t)-\bar x\|_2< \epsilon$$
for all $t \geq t_0$.
If the equilibrium solution is not locally stable it is said to be \emph{unstable}.
\end{definition}

Euclidian distance between two points $Y_1=(y_1^1,y_2^1,\dots,y_n^1)$ and $Y_2=(y_1^2,y_2^2,\dots,y_n^2)$ in $\mathbb{R}^n$ is $$\|Y_1-Y_2\|_2=\sqrt{\sum _{i=1}^n(y_i^1-y_i^2)^2}.$$

\begin{definition}(Local asymptotic stability) 
An equilibrium solution $\bar x$ of \eqref{eq:1st_order_ODE} is \emph{locally asymptotically stable} if it is locally stable and if there exist $\gamma >0$ such that $\| x_0-\bar x\| _2< \gamma$ implies
$$\lim_{t\rightarrow \infty}\| x(t)-\bar x\| _2=0.$$
\end{definition}


\begin{definition}(Periodic solution)
A \emph{periodic solution} of the system \eqref{eq:1st_order_ODE} is a nonconstant solution $x(t)$ satisfying $x(t+T)=x(t)$ for all $t$ on the interval of existence  for some $T>0$. The minimum value of $T$ is called the period of the solution.
\end{definition}

\section{First-order differential equations}
\begin{definition}
The standard form of first order linear
equations is
$$\frac{dy}{dt}+p(t)y=g(t)$$
$p$ and $g$ are given functions of the independent variable $t$.
\end{definition}


\subsection{Analytical methods}


{\bf Linear equations: Integrating factors}



To solve $1^{st}$ order linear equation with non-constant coefficients (this method can also be used for equation with constant coefficient).
\begin{enumerate}
\item Put the DE in the standard form
\begin{equation}\frac{dy}{dt}+p(t)y=g(t)\label{eq:DE}\end{equation}
\item Determine the integrating factor $\mu (t)$
\begin{itemize}
\item Multiply the DE (\ref{eq:DE}) by $\mu (t)$
\begin{equation}\mu (t)\frac{dy}{dt}+\mu (t)p(t)y=\mu
(t)g(t)\label{eq:DE1}\end{equation} 
\item  State that the left side of (\ref{eq:DE1}) is equal to $\frac{d}{dt}(\mu (t)y)$
$$\frac{d}{dt}(\mu (t)y)=\mu (t)\frac{dy}{dt}+y\frac{d \mu}{dt}=\mu (t)\frac{dy}{dt}+\mu
(t)p(t)y$$
\item Solve for $\mu (t)$
$$\frac{d \mu }{dt}=\mu (t) p(t)$$
\end{itemize}
$$\Rightarrow \quad \mu(t)=e^{\int p(t)dt}$$
\item Solve (\ref{eq:DE1}) for $y$ with $\mu(t)=e^{\int p(t)dt}$
$$\left(\frac{d}{dt}e^{\int p(t)dt}y=\right)e^{\int p(t)dt}\frac{dy}{dt}+p(t)e^{\int p(t)dt}y=e^{\int p(t)dt}g(t)$$
$$\frac{d}{dt}\mu(t)y=\mu(t)g(t)$$
$$\mu(t)y=\int \mu(t)g(t)dt +c$$
\end{enumerate}

Hence the general solution of (\ref{eq:DE}) is
$$y(t)=\frac{1}{\mu(t)}\left [\int_{t_0}^t \mu(s)g(s)ds +c\right ]
\quad with \quad \mu(t)=e^{\int p(t)dt}$$




{\bf Separable equations}



\begin{definition}(Separable equations) A first order differential equation
$$\frac{dy}{dx}=f(x,y)$$
is said to be separable or to have separable variables if it can be
expressed as follows
$$\frac{dy}{dx}=g(x)h(y)$$
(the rate function can be expressed as a product of a function of
the independent variable times a function of the dependent variable
).
\end{definition}


{\bf To solve separable equations: }
$\frac{dy}{dx}=g(x)h(y)$
\begin{enumerate}
\item Express the separable equation as follows
$$\frac{1}{h(y)}\frac{dy}{dx}=g(x)$$
\item As $y$, $\frac{dy}{dx}$, and $g(x)$ are functions of $x$,
apply the integral
$$\int \frac{1}{h(y)}\frac{dy}{dx} dx=\int g(x) dx$$
\item Use the Change of variable Theorem (if $u=v(x)$ $\int f(v(x))v'(x)dx=\int
f(u)du$) for the left side with $u=y(x)$
$$\int \frac{1}{h(u)}du=\int g(x) dx$$
$$\int \frac{1}{h(y)}dy=\int g(x) dx$$
\item Integrate
\begin{equation}H(y)=G(x)+c\label{eq:GS}\end{equation} $c$ is the combination of the left
and right integration constants, $H$ and $G$ are antiderivatives of
$\frac{1}{h(y)}$ and $g(x)$ respectively. \item Solve equation
(\ref{eq:GS}) for $y$ to obtain a explicit form of the general
solution.
\end{enumerate}



\subsubsection{Local stability}
A simple criterion for determining the L.A.S. of an equilibrium solution to a first order autonomous differential equation
\begin{equation}
\label{eq:ODE1Nonlin}
\frac{dy}{dt}=f(y)
\end{equation}
having an equilibrium at $\bar y$.


\begin{theorem}
Suppose $f'$ is continuous on an open interval $I$ containing $\bar y$, where $\bar y$ is an equilibrium of $\frac{dy}{dt}=f(y)$. Then $\bar y$ is locally asymptotically stable if $$f'(\bar y)<0$$
and unstable if $f'(\bar y)>0$.
\end{theorem}

The value $f'(\bar y)$ is know as the eigenvalue of the linearized equation.


\begin{definition}
The equilibrium $\bar y$ of $\frac{dy}{dt}=f(y)$ is called hyperbolic if $f'(\bar y)\not =0$. Otherwise, it is called nonhyperbolic.
\end{definition}


Nonhyperbolic equilibria have a zero eigenvalue, and hence their local stability is indeterminate.


\subsubsection{Phase line analysis}
Consider a first-order nonautonomous equation
$$\frac{dy}{dt}=f(t,y).$$
$f(t,y)$ represents the slope of the tangent to the solution $y(t)$ at the point $(t,y)$. Hence $f(t,y)$ can be used to construct a direction field in the $t,y-$plane.

Consider a first-order autonomous equation
$$\frac{dy}{dt}=f(y).$$
Here the direction of flow does not change with $t$. Hence it is only necessary to determine the direction of flow on the $y-$axis (Phase line). If $\frac{dy}{dt}$ is positive, the direction of flow is in the positive direction, if it is negative, the flow is in negative direction.



\subsection{Higher-order linear equations}
The general solution to an $n^{th}-$order linear nonhomogeneous differential equation is the sum of two solutions, a general solution to the homogeneous differential equation and a particular solution to the nonhomogeneous differential equation
$$y(t)=y_h(t)+y_p(t).$$ 



{\bf $\bullet$} Methodology to find the general solution to the homogeneous equation is given for second-order linear equations, but it can be generalized to the $n^{th}-$order linear equations.


\begin{theorem} (Principle of superposition)
If $y_1$ and $y_2$ are two solutions of the homogeneous linear differential
equation on an interval $I$


$$y''+p(t)y'+q(t)y=0$$

then the linear combination $$c_1y_1+c_2y_2$$ is also a solution for
any values of the constants $c_1$ and $c_2$ on the interval.
\end{theorem}

\begin{definition}(Wronskian)
Suppose each of the functions $f_1(t),
f_2(t), \dots, f_n(t)$ has at least $n-1$ derivatives. The
determinant

$$W(f_1,f_2,\dots,f_n)=\left |
\begin{array}{llll}
f_1 & f_2 & \cdots & f_n\\
f_1' & f_2' & \cdots & f_n'\\
\vdots & \vdots & &\vdots\\
f_1^{(n-1)} & f_2^{(n-1)} & \cdots & f_n^{(n-1)}\\
\end{array}
\right |$$ is called the Wronskian of the functions.
\end{definition}


\begin{theorem}
Let $y_1$ and $y_2$ be solutions of the differential equation
$$
y''+p(t)y'+q(t)y=0$$ where $p$ and $q$ are continuous on an open
interval $I$. Then $y_1$ and $y_2$ are linearly independent on $I$
if and only if $W(y_1,y_2)(t)\not=0$ $\forall t \in I$.
\end{theorem}

\begin{definition}
Any set $y_1$, $y_2$ of 2 linearly independent solutions of
$$
y''+p(t)y'+q(t)y=0$$ on an open interval $I$ is said to be {\bf a
fundamental set of solutions} on $I$ of the differential equation.
\end{definition} 


\begin{definition}
The general solution of
$$y''+p(t)y'+q(t)y=0$$ on an open interval $I$ is the linear combination of 2 linearly independent solutions $y_1$ and $y_2$
$$y(t)=c_1 y_1(t)+c_2y_2(t)$$
with $c_1$ and $c_2$ constants. $y_1$ and $y_2$ form a fundamental set of solutions.
\end{definition}


{\bf Method for linear homogeneous
second-order equations with constant coefficients}
\begin{equation}
ay''+by'+cy=0\label{eq:constant}
\end{equation}
Let assume that the solution can take the form of $$y(t)=e^{rt}.$$
\vspace{-.2cm}
 \begin{enumerate}
\item Write the {\bf characteristic equation }
$$ar^2+br+c=0$$
\item Find the {\bf roots} $r_1$ and $r_2$ of the characteristic equation
$$r_1=\frac{-b-\sqrt{\Delta}}{2a} , \qquad r_2=\frac{-b+\sqrt{\Delta}}{2a}, \quad \textrm{with} \quad \Delta=b^2-4ac $$
\begin{description}
\item[$\Delta >0$]: $r_1$, $r_2$ distinct and real,
$y_1(t)=e^{r_1t}$ and $y_2(t)=e^{r_2t}$ are 2 linearly independent
solutions of \eqref{eq:constant}. The general solution is
$$y(t)=c_1 e^{r_1t}+c_2e^{r_2t}, \quad c_1, c_2\textrm{ arbitrary constants}$$
\item[$\Delta =0$]: $r_1=r_2=r$ real, $y_1(t)=e^{rt}$ and $y_2(t)=te^{rt}$ are 2 linearly independent
solutions of \eqref{eq:constant}. The general solution is
$$y(t)=c_1 e^{rt}+c_2te^{rt} , \quad c_1, c_2\textrm{ arbitrary constants}$$
\item[$\Delta <0$]: $r_1$, $r_2$ complex conjugates, $r_1=\alpha + i \mu$ and $r_2=\alpha - i
\mu$. $y_1(t)=e^{\alpha t}\cos (\mu t)$ and $y_2(t)=e^{\alpha t}\sin
(\mu t)$ are 2 linearly independent solutions of
\eqref{eq:constant}. The general solution is
$$y(t)=c_1 e^{\alpha t}\cos (\mu t)+c_2e^{\alpha t}\sin (\mu t) , \quad c_1, c_2\textrm{ arbitrary constants}$$
\end{description}
\end{enumerate}

Note that an $n^{th}-$order linear homogeneous differential equation always has a solution equal to zero $y(t)\equiv 0$. When the equation has constant coefficients, the stability of the zero solution is stable (when a solution to an IVP will tend to zero). Stability of the zero solution depends on the eigenvalues, the roots of $\lambda_i$ of the characteristic equation.

\begin{theorem}
If all of the roots of the characteristic polynomial $P(\lambda)$ are negative or have negative real part, then given any solution $y(t)$ of the homogeneous differential equation $$\frac{d^n y}{dy^n}+a_1(t)\frac{d^{n-1}y}{dy^{n-1}}+\dots+a_{n-1}(t)\frac{dy}{dy}+a_n(t)y=0,$$ there exist positive constant $M$ and $b$ such that
$$|y(t)|\leq Me^{-bt}\quad for \quad t>0$$
and
$$\lim _{t\rightarrow \infty} |x(t)|=0$$
\end{theorem}




Methodologies to find a particular solution to the nonhonogeneous differential equation:
\begin{itemize}
\item Undetermined coefficients
\item Variation of parameters
\end{itemize}


{\bf Undetermined coefficients (to find
a particular solution $Y(t)$)}
\begin{itemize}
\item Make an assumption about the form of the particular solution
$Y(t)$ but with the coefficients unspecified
\item The particular solution has to satisfy the nonhomogeneous
equation $\Leftrightarrow$ Substitute the assumed expression of
$Y(t)$ in the nonhomogeneous equation $\Rightarrow$ Coefficients to
be determined
\end{itemize}
Conditions for application: 
if $g(t)$ takes the form: constant function, exponential, polynomial, sine, cosine, any sum or products of such functions.
$$y''+p(t)y'+q(t)y=g(t)$$
\begin{itemize}
\item Make an assumption about the form of the particular solution
 $Y(t)$: if $g(t)$ is of the form in the left column or is the sum or product of such function, then check a particular solution $Y(t)$ of the corresponding form as indicated in the right column
\begin{center}
\begin{tabular}{l|l}
$\mathbf{g(t)}$ & {\bf Assumed form of }$Y(t)$\\
$\alpha e^{\beta t}$& $ae^{\beta t}$\\
$\alpha \cos (\omega t) +\beta \sin (\omega t)$ & $a \cos (\omega t)
+b \sin (\omega t)$\\
$\alpha$ & $a$\\
$\alpha + \beta t$ & $a +bt$\\
$\alpha + \beta t +\gamma t^2$ & $a +bt +ct^2$\\
$\alpha + \beta t +\gamma t^2+ \cdots+\delta t^m$ & $a +bt +ct^2+ \cdots+d t^m$\\
\end{tabular}
\end{center}
\item If coefficients cannot be determined, then no solution of this
form exists $\Rightarrow$ modify the assumption
\item If $g(t)$ contains terms that duplicate any solutions of the corresponding homogeneous equation
$\Rightarrow$ each such term must be multiplied by $t^s$
($s$ the smallest natural number that eliminates the
duplication).
\end{itemize}


{\bf Variation of parameters: }
$y''+p(t)y'+q(t)y=g(t)$
\begin{enumerate}
\item Find the {\bf general solution of the homogeneous
equation} $\Leftrightarrow$ Find 2 linearly independent solutions
$y_1$ and $y_2$ of the homogeneous equation
\item Check for the nonhomogeneous equation a solution of the form
$$Y(t)=u_1(t)y_1(t)+u_2(t)y_2(t)$$
where $u_1$ and $u_2$ are two functions of $t$ to be determined, and
that satisfy the {\bf second condition}
$$u_1'(t)y_1(t)+u_2'(t)y_2(t)=0$$
\item Substitute $Y(t)$ in the nonhomogeneous equation and use the
second condition to obtain
$$u'_1(t)y_1'(t)+u_2'(t)y_2'(t)=g(t)$$
\item Solve  the system of 2 linear algebraic
equation for $u_1'$ and $u_2'$
$$\left \{
\begin{array}{ll}
u_1'(t)y_1(t)+u_2'(t)y_2(t)&=0\\
u'_1(t)y_1'(t)+u_2'(t)y_2'(t)&=g(t)
\end{array}\right . \mathrm{or} \quad
\left [
\begin{array}{ll}
y_1(t)&y_2(t)\\
y_1'(t)&y_2'(t)
\end{array}\right ]\left [
\begin{array}{l}
u_1'(t)\\
u_2'(t)
\end{array}\right ]=\left [
\begin{array}{l}
0\\
g(t)
\end{array}\right ]
$$

$$\Rightarrow u_1'(t)=\frac{-y_2(t)g(t)}{W(y_1,y_2)(t)}, \qquad u_2'(t)=\frac{y_1(t)g(t)}{W(y_1,y_2)(t)}$$
\item Integrate, evaluate the integral omitting the integration
constants
$$\Rightarrow u_1(t)=-\int \frac{y_2(t)g(t)}{W(y_1,y_2)(t)}dt, \qquad u_2(t)=\int \frac{y_1(t)g(t)}{W(y_1,y_2)(t)}dt$$
\item A {\bf particular solution} of the nonhomogeneous equation is
$$Y(t)=-y_1(t)\int \frac{y_2(t)g(t)}{W(y_1,y_2)(t)}dt + y_2(t)\int \frac{y_1(t)g(t)}{W(y_1,y_2)(t)}dt$$
\item The {\bf general solution} of the nonhomogeneous equation is
$$y(t)=c_1y_1(t)+c_2y_2(t)+Y(t), \qquad c_1, \; c_2 \; \textrm{arbitrary constants}$$
\end{enumerate}


\begin{theorem}
If the functions $p$, $q$ and $g$ are continuous on an open interval
$I$, and if the functions $y_1$ and $y_2$ are linearly independent
solutions of the homogeneous equation, $y''+p(t)y'+q(t)y=0$,
corresponding to the nonhomogeneous equation
$$y''+p(t)y'+q(t)y=g(t)$$
then a particular solution of the nonhomogeneous equation is
$$Y(t)=-y_1(t)\int_{t_0}^t \frac{y_2(s)g(s)}{W(y_1,y_2)(s)}ds+y_2(t)\int_{t_0}^t \frac{y_1(s)g(s)}{W(y_1,y_2)(s)}ds$$
where $t_0$ is any point in $I$. The general solution is
$$y(t)=c_1y_1(t)+c_2y_2(t)+Y(t).$$
\end{theorem}

\section{Systems of linear equations}
A $n-$dimensional linear system
$$\frac{dX}{dt}=A X$$
where $A$ is a $n\times n$ constant matrix with real elements. The general solution of this system is
$$X(t)=e^{At}C$$
where $e^{At}$ is a $n\times n$ matrix known as the fundamental matrix and $C$ ia a $n\times 1$ vector. There exist many methods to compute the matrix exponential. To compute the general solution of the $n-$dimensional system, a straightforward method can be used (similar method used for higher-order constant coefficient DE).




Here the case of a 2-dimensional linear system is studied. 
Consider a system of first-order linear equations
\begin{equation}
\frac{dX}{dt}=A X
\label{eq:SysLin}
\end{equation}
where $X=(x_1,x_2)^T$ and $A$ is a constant $2\times 2$ matrix. Note that the zero solution $X=0$ is solution of the differential equation. 

Let $X=e^{\lambda t}V$ be a solution of \eqref{eq:SysLin}, then
$$AV=\lambda V$$
where $\lambda $ is the eigenvalue of $A$ and $V$ is the eigenvector corresponding to $\lambda$. The eigenvalues are solutions of $$\det (A-\lambda I)=0.$$
Then
$$\lambda ^2 -\tr (A) \lambda + \det (A)=0.$$
Solutions of \eqref{eq:SysLin} can take 3 different forms
\begin{itemize}
\item if eigenvalues are real and distinct
$$X(t)=c_1V_1 e^{\lambda _1t}+c_2V_2 e^{\lambda _2t}$$
with $c_1$ and $c_2$ constant.
\item if eigenvalues are real and equal
$$X(t)=c_1V_1 e^{\lambda _1t}+c_2\left (V_1te^{\lambda _1t}+P e^{\lambda _1t}\right )$$
with $c_1$ and $c_2$ constant. $P$ can be obtained by solving $(A-\lambda _1 I)P=V_1$.
\item if eigenvalues are complex conjugate $\lambda _{1,2}=a\pm i b$
$$X(t)=P_1 e^{at} \cos (bt) + P_2 e^{at}\sin (bt).$$
\end{itemize}
Behaviors of solutions:
\begin{itemize}
\item The origin is asymptotically stable if the eigenvalues of $A$ are negative or have negative real part.
\item The origin is stable if the eigenvalues of $A$ are nonpositive or have nonpositive real part.
\item The origin is unstable if the eigenvalues of $A$ are positive or have positive real part.
\end{itemize}


\begin{theorem}
Suppose $\frac{dX}{dt}=A X$ where $A$ is a constant $2\times 2$ matrix with $\det (A) \not =0$. 

The orign is asymptotically stable iff
$$\tr(A)<0 \quad and \quad \det (A)>0.$$

The orign is stable iff
$$\tr(A)\leq 0 \quad and \quad \det (A)>0.$$


The origin is unstable iff 
$$\tr(A)> 0 \quad or \quad \det (A)<0.$$
\end{theorem}


In the case of real eigenvalues $\lambda _1$ and $\lambda _2$, the eigenvectors $V_1$ and $V_2$ are directions along which solutions travel toward or away the origin:
\begin{itemize}
\item if $\lambda _1$ is positive, solutions will travel along $V_1$ away from the origin.
\item if $\lambda _2$ is negative, solutions will travel along $V_2$ toward the origin.
\end{itemize}
Hence solutions travel in a direction which corresponds to a linear combination of $V_1$ and $V_2$
\begin{theorem}
Consider a system of first order linear equations
$$\frac{dX}{dt}=A X$$
where $X=(x_1,x_2)^T$ and $A$ is a $2\times 2-$matrix.
\begin{itemize}
\item if $\det A >0$ and $\tr A ^2 -4 \det A\geq 0$ then the origin is a node (real eigenvalues having the same signs); a stable node if $\tr A <0$ (real $\lambda _{1,2}<0$), and an unstable node if $\tr A >0$ (real $\lambda _{1,2}>0$).
\item if $\det A <0$ then the origin is a saddle (real eigenvalues have opposite signs, $\lambda _1 \lambda _2<0$).
\item if $\det A >0$ and $\tr A ^2 -4 \det A < 0$ and $\tr A \not =0$, the origin is a spiral (complex conjugate with nonzero real part); it is stable if $\tr A <0$ (negative real part) and unstable if $\tr A >0$ (positive real part).
\item if $\det A >0$ and $\tr A=0$ then the origin is a center (purely imaginary eigenvalues $\lambda _{1,2}=\pm ib$).
\end{itemize}
\end{theorem}



\section{Linear systems of ODE}
\begin{definition}[Linear ODE]
A \emph{linear} ODE is a differential equation taking the form
\begin{equation}\label{sys:linear_general}
\frac{d}{dt}x=A(t)x+B(t),
\end{equation}
where $A(t)\in\mathcal{M}_n(\IR)$ with continuous entries, $B(t)\in\IR^n$ with real valued, continuous coefficients, and $x\in\IR^n$. The associated IVP takes the form 
\begin{equation}\label{sys:IVP_linear_general}
\begin{aligned}
\frac{d}{dt}x &= A(t)x+B(t) \\
x(t_0)&=x_0.
\end{aligned}
\end{equation}
\end{definition}

\paragraph{Types of systems:}
\begin{itemize}
\item $x'=A(t)x+B(t)$ is linear nonautonomous ($A(t)$ depends on $t$) nonhomogeneous (also called \emph{affine} system).
\item $x'=A(t)x$ is linear nonautonomous homogeneous.
\item $x'=Ax+B$, that is, $A(t)\equiv A$ and $B(t)\equiv B$, is linear autonomous nonhomogeneous (or affine autonomous).
\item $x'=Ax$ is linear autonomous homogeneous.
\end{itemize}
\begin{itemize}
\item If $A(t+T)=A(t)$ for some $T>0$ and all $t$, then linear periodic.
\end{itemize}



\begin{theorem}[Existence and Uniqueness]
Solutions to \eqref{sys:IVP_linear_general} exist and are unique on the whole interval over which $A$ and $B$ are continuous.
In particular, if $A,B$ are constant, then solutions exist on $\IR$.
\end{theorem}


\frame{\frametitle{Autonomous linear systems}
Consider the autonomous affine system
\begin{equation}\label{sys:affine_auton}
\frac{d}{dt}x=Ax+B,
\end{equation}
and the associated homogeneous autonomous system
\begin{equation}\label{sys:lin_auton}
\frac{d}{dt}x=Ax.
\end{equation}
}

\subsection{Exponential of a matrix}
\begin{definition}[Matrix exponential]
Let $A\in\M_n(\IK)$ with $\IK=\IR$ or $\IC$. The \emph{exponential} of $A$, denoted $e^{At}$, is a matrix in $\M_n(\IK)$, defined by
\[
e^{At}=\II +\sum_{k=1}^\infty \frac{t^k}{k!}A^k,
\]
where $\II$ is the identity matrix in $\M_n(\IK)$.
\end{definition}

\paragraph{Properties of the matrix exponential}
\begin{itemize}
\item $e^{At_1}e^{At_2}=e^{A(t_1+t_2)}$ for all $t_1,t_2\in\IR$.
\item $Ae^{At}=e^{At}A$ for all $t\in\IR$. [$A$ and $e^{At}$ commute]
\item $(e^{At})^{-1}=e^{-At}$ for all $t\in\IR$.
\end{itemize}
\begin{theorem}\label{th:solution_matrix_exponential}
The unique solution $\phi$ of \eqref{sys:lin_auton} with initial condition $\phi(t_0)=x_0$ is given by
\[
\phi(t)=e^{A(t-t_0)}x_0.
\]
\end{theorem}


\subsection{Computing the matrix exponential}
Let $P$ be a nonsingular matrix in $\M_n(\IR)$. We transform the IVP
\begin{equation}\label{IVP:lin_auton}
\begin{aligned}
\frac{d}{dt}x &= Ax\\
x(t_0)&=x_0,
\end{aligned}
\end{equation}
using the transformation $x=Py$ or $y=P^{-1}x$.
The dynamics of $y$ is
\begin{align*}
y' &= (P^{-1}x)' \\
&= P^{-1}x' \\
&= P^{-1}Ax \\
&= P^{-1}APy.
\end{align*}
The initial condition is $y_0=P^{-1}x_0$.
We have thus transformed IVP \eqref{IVP:lin_auton} into
\begin{equation}\label{IVP:lin_auton2}
\begin{aligned}
\frac{d}{dt}y &= P^{-1}APy\\
y(t_0)&=P^{-1}x_0.
\end{aligned}
\end{equation}
By Theorem~\ref{th:solution_matrix_exponential}, the solution of \eqref{IVP:lin_auton2} is given by
\[
\psi(t)=e^{P^{-1}AP(t-t_0)}P^{-1}x_0,
\]
and since $x=Py$, the solution to \eqref{IVP:lin_auton} is given by
\[
\phi(t)=Pe^{P^{-1}AP(t-t_0)}P^{-1}x_0.
\]
So everything depends on $P^{-1}AP$.



\subsection{Matrix exponential -- Diagonalizable case}
Before we begin, recall the following definitions and results (see for example \cite{Franklin2000}).
\begin{definition}
Let $A,B\in\M_n(\IK)$. $A$ and $B$ are \emph{similar} if there exists $P\in\M_n(\IK)$ such that $P^{-1}AP=B$.
\end{definition}
\begin{theorem}
A matrix $A\in\M_n(\IK)$ is similar to a diagonal matrix if and only if $A$ has $n$ linearly independent eigenvectors. In particular, if $A$ has distinct eigenvalues, then $A$ is similar to a diagonal matrix.
\end{theorem}
Assume $P$ nonsingular in $\M_n(\IR)$ such that
\[
P^{-1}AP=
\begin{pmatrix}
\lambda_1 & & 0 \\
& \ddots & \\
0 & & \lambda_n
\end{pmatrix}
\]
with all eigenvalues $\lambda_1,\ldots,\lambda_n$ different.
We have
\[
e^{P^{-1}AP}=\II+\sum_{k=1}^\infty\frac{t^k}{k!}
\begin{pmatrix}
\lambda_1 & & 0 \\
& \ddots & \\
0 & & \lambda_n
\end{pmatrix}^k
\]
For a (block) diagonal matrix $M$ of the form
\[
M=
\begin{pmatrix}
m_{11} & & 0\\
& \ddots & \\
0 & & m_{nn}
\end{pmatrix}
\]
there holds
\[
M^k=
\begin{pmatrix}
m_{11}^k & & 0\\
& \ddots & \\
0 & & m_{nn}^k
\end{pmatrix}
\]
Therefore,
\begin{align*}
e^{P^{-1}AP} &= \II+\sum_{k=1}^\infty\frac{t^k}{k!}
\begin{pmatrix}
\lambda_1^k & & 0 \\
& \ddots & \\
0 & & \lambda_n^k
\end{pmatrix} \\
&=\begin{pmatrix}
\sum_{k=0}^\infty\frac{t^k}{k!}\lambda_1^k & & 0 \\
& \ddots & \\
0 & & \sum_{k=0}^\infty\frac{t^k}{k!}\lambda_n^k
\end{pmatrix}\\
&=\begin{pmatrix}
e^{\lambda_1t} & & 0 \\
& \ddots & \\
0 & & e^{\lambda_nt}
\end{pmatrix}
\end{align*}
And so the solution to \eqref{IVP:lin_auton} is given, in the case that $A$ is diagonalizable, by
\begin{equation}\label{sol:linear_sys_ODE_diagonalizable}
\phi(t)=P
\begin{pmatrix}
e^{\lambda_1t} & & 0 \\
& \ddots & \\
0 & & e^{\lambda_nt}
\end{pmatrix}
P^{-1}x_0.
\end{equation}



\subsection{Matrix exponential -- Nondiagonalizable case}
\begin{definition}[Generalized eigenvectors]
Let $A\in\M_n(\IR)$. Suppose $\lambda$ is an eigenvalue of $A$ with multiplicity $m\leq n$. Then, for $k=1,\ldots,m$, any nonzero solution $v$ of
\[
(A-\lambda\II)^kv=0
\]
is called a \emph{generalized eigenvector} of $A$.
\end{definition}


\begin{definition}[Nilpotent matrix]
Let $A\in\M_n(\IR)$. $A$ is \emph{nilpotent} (of order $k$) if $A^{j}\neq 0$ for $j=1,\ldots,k-1$, and $A^k=0$.
\end{definition}


\begin{theorem}[Jordan normal form]
\label{th:Jordan_form}
Let $A\in\M_n(\IR)$ have eigenvalues $\lambda_1,\ldots,\lambda_n$, repeated according to their multiplicities. 
\begin{itemize}
\item 
Then there exists a basis of generalized eigenvectors for $\IR^n$. 
\item 
And if $\{v_1,\ldots,v_n\}$ is any basis of generalized eigenvectors for $\IR^n$, then the matrix $P=[v_1\cdots v_n]$ is invertible, and $A$ can be written as
\[
A=S+N,
\]
where
\[
P^{-1}SP=\diag(\lambda_j),
\]
the matrix $N=A-S$ is nilpotent of order $k\leq n$, and $S$ and $N$ commute, i.e., $SN=NS$.
\end{itemize}
\end{theorem}
Another formulation of the same theorem, which is ``self-contained'', is as follows.
\begin{theorem}
Let $A\in\M_n(\IR)$ with eigenvalues $\lambda_1,\ldots,\lambda_s$ with multiplicities $m_1,\ldots,m_s$:
\[
\det\left(A-\lambda I\right)=\prod_{j=1}^s(\lambda-\lambda_j)^{m_j}.
\]
Then $A$ is similar to a matrix of the form
\[
J=\diag\left(\Lambda_1,\ldots,\Lambda_s\right),
\]
where each block $\lambda_i$ is an $m_i\times m_i$-matrix of the form
\[
\Lambda_i=
\begin{pmatrix}
\lambda_i & \ast & 0 &\cdots & 0 \\
0 & \lambda_i & \ast & \cdots & 0 \\
 & & & &  \\
0 & & & \lambda_i & \ast \\
& & & 0 & \lambda_i
\end{pmatrix}.
\]

\end{theorem}


The Jordan canonical form is
\[
P^{-1}AP=
\begin{pmatrix}
J_0 & & 0\\
& \ddots & \\
0 & & J_s
\end{pmatrix},
\]
so we use the same property as before (but with block matrices now), and
\[
e^{P^{-1}APt}=
\begin{pmatrix}
e^{J_0t} & & 0\\
& \ddots & \\
0 & & e^{J_st}
\end{pmatrix}.
\]
The first block in the Jordan canonical form takes the form
\[
J_0=
\begin{pmatrix}
\lambda_0 & & 0\\
& \ddots & \\
0 & & \lambda_k
\end{pmatrix}
\]
and thus, as before,
\[
e^{J_0t}=
\begin{pmatrix}
e^{\lambda_0t} & & 0\\
& \ddots & \\
0 & & e^{\lambda_kt}
\end{pmatrix}.
\]
Other blocks $J_i$ are written as
\[
J_i=\lambda_{k+i}\II+N_i,
\]
with $\II$ the $n_i\times n_i$ identity and $N_i$ the $n_i\times n_i$ nilpotent matrix
\[
N_i=\begin{pmatrix}
0 & 1 & 0 && 0 \\
&&\ddots && \\
&&&& 1 \\
0& &&&0
\end{pmatrix}.
\]
$\lambda_{k+i}\II$ and $N_i$ commute, and thus
\[
e^{J_it}=e^{\lambda_{k+i}t}e^{N_it}.
\]
Since $N_i$ is nilpotent, $N_i^k=0$ for all $k\geq n_i$. Therefore, the series $e^{N_it}$ terminates and
\[
e^{J_it}=e^{\lambda_{k+i}t}
\begin{pmatrix}
1 & t & \cdots & \frac{t^{n_i-1}}{(n_i-1)!} \\
0 & 1 & \cdots & \frac{t^{n_i-2}}{(n_i-2)!} \\
&&& \\
0 &&& 1
\end{pmatrix}.
\]


\begin{theorem}
For all $(t_0,x_0)\in\IR\times\IR^n$, there is a unique solution $x(t)$ to \eqref{IVP:lin_auton} defined for all $t\in\IR$. Each coordinate function of $x(t)$ is a linear combination of functions of the form
\[
t^ke^{\alpha t}\cos(\beta t)\quad\textrm{and}\quad t^ke^{\alpha t}\sin(\beta t)
\]
where $\alpha+i\beta$ is an eigenvalue of $A$ and $k$ is less than the algebraic multiplicity of the eigenvalue.
\end{theorem}
\begin{theorem}
Under conditions of the Jordan normal form Theorem~\ref{th:Jordan_form}, the linear system $x'=Ax$ with initial condition $x(0)=x_0$, has solution
\[
x(t)=P\diag\left(e^{\lambda_j t}\right)P^{-1}\left(\II+Nt+\cdots\frac{t^k}{k!}N^k\right) x_0.
\]
\end{theorem}
The result is particularly easy to apply in the following case, where we do not need the matrix $P$ (the basis of generalized eigenvectors).
\begin{theorem}[Case of an eigenvalue of multiplicity $n$]
\label{th:Jordan_simple_case}
Suppose that $\lambda$ is an eigenvalue of multiplicity $n$ of $A\in\M_n(\IR)$. Then $S=\diag(\lambda)$, and the solution of $x'=Ax$ with initial value $x_0$ is given by
\[
x(t)=e^{\lambda t}\left(\II+Nt+\cdots\frac{t^k}{k!}N^k\right) x_0.
\]
\end{theorem}


Finally, we give the following variation of constants formula, using the matrix exponential.
\begin{theorem}[Variation of constants formula]
Consider the IVP
\begin{subequations}\label{sys:lin_nonlin}
\begin{align}
x' &= Ax+B(t) \\
x(t_0) &= x_0,
\end{align}
\end{subequations}
where $B:\IR\to\IR^n$ a smooth function on $\IR$, and let $e^{A(t-t_0)}$ be matrix exponential associated to the homogeneous system $x'=Ax$. Then the solution $\phi$ of \eqref{sys:lin_nonlin} is given by
\begin{equation}
\phi(t)=e^{A(t-t_0)}x_0 + \int_{t_0}^t e^{A(t-s)}B(s)ds.
\end{equation}
\end{theorem}







\section{The Laplace transform}

\begin{definition}[Laplace transform]
Let $f(t)$ be a function defined for $t\geq 0$. The \emph{Laplace transform} of $f$ is the function $F(s)$ defined by
\[
F(s)=\L\{f(t)\}=\int_0^\infty e^{-st}f(t)dt.
\]
\end{definition}
The Laplace transform is a linear operator:
\[
\L\{af(t)+bg(t)\}=a\L\{f(t)\}+b\L\{g(t)\}.
\]


We have the following rules of transformation:
\begin{center}
\begin{tabular}{ll}
$t$-domain & $s$-domain\\
\hline
$af(t)+bg(t)$ & $aF(s)+bG(s)$ \\
$tf(t)$ & $-F'(s)$ \\
$t^nf(t)$ & $(-1)^nF^{(n)}(s)$ \\
$f'$ & $sF(s)-f(0)$ \\
$f''$ & $s^2F(s)-sf(0)-f'(0)$ \\
$f^{(n)}$ & $s^nF(s)-s^{n-1}f(0)-\cdots-f^{(n-1)}(0)$ \\
$\frac{f(t)}{t}$ & $\int_s^\infty F(u)du$ \\
$\int_0^t f(u)du=u(t)\ast f(t)$ & $\frac 1s F(s)$ \\
$f(at)$ & $\frac 1{|a|}F\left(\frac sa\right)$ \\
$e^{at}f(t)$ & $F(s-a)$ \\
$f(t-a)u(t-a)$ & $e^{-as}F(s)$ \\
$f(t)\ast g(t)$ & $F(s)G(s)$
\end{tabular}
\end{center}
Here $f^{(n)}$ represents the $n$th derivative, not the $n$th iterate, and $\ast$ is the convolution product,
\[
(f\ast g)(t)=\int_a^b f(s)g(t-s)ds.
\]


\frame{\frametitle{Dirac delta -- Heaviside function}
In the table on the following slide,
\begin{itemize}
\item $\delta(t)$ is the Dirac delta,
\[
\delta(t)=\begin{cases}
\infty & \textrm{if }t=0\\
0 & \textrm{if }t\neq 0.
\end{cases}
\]
\item $H(t)$ is the Heaviside function,
\[
H(t)=\begin{cases}
0 & \textrm{if }t< 0 \\
1 & \textrm{if }t> 0.
\end{cases}
\]
\end{itemize}
Note that $H(t)=\int_{-\infty}^t \delta(s)ds$.
}

\frame{\frametitle{Transforms of common functions}
\begin{center}
\begin{tabular}{ll}
$t$-domain & $s$-domain\\
\hline
$\delta(t)$ & $1$ \\
$\delta(t-\tau)$ & $e^{-\tau s}$ \\
$H(t)$ & $\frac 1s$ \\
$H(t-\tau)$ & $\frac{e^{-\tau s}}s$ \\
$\frac{t^n}{n!}H(t)$ & $\frac{1}{s^{n+1}}$ \\
$e^{-\alpha t}H(t)$ & $\frac 1{s+\alpha}$ \\
$\sin(\omega t)H(t)$ & $\frac{\omega}{s^2+\omega^2}$ \\
$\cos(\omega t)H(t)$ & $\frac s{s^2+\omega^2}$ \\
\end{tabular}
\end{center}
}

\begin{definition}
Given a function $F(s)$, if there exists $f(t)$, continuous on $[0,\infty)$ and such that
\[
\L\{f\}=F,
\]
then $f(t)$ is the \emph{inverse Laplace transform} of $F(s)$, and is denoted $f=\L^{-1}\{F\}$.
\end{definition}
\begin{theorem}
The inverse Laplace transform is a linear operator. Assume that $\L^{-1}\{F_1\}$ and $\L^{-1}\{F_2\}$ exist, then
\[
\L^{-1}\{aF_1+bF_2\}=a\L^{-1}\{F_1\}+b\L^{-1}\{F_2\}.
\]
\end{theorem}

\paragraph{Solving differential equations using the Laplace transform}
\begin{enumerate}
\item Take the Laplace transform of both sides of the equation.
\item Using the initial conditions, deduce an algebraic system of equations in $s$-space.
\item Solve the algebraic system in $s$-space.
\item Take the inverse Laplace transform of the solution in $s$-space, to obtain the solution of the differential equation in $t$-space.
\end{enumerate}












\section{Systems of nonlinear equations}
\begin{theorem}(Existence and Uniqueness)
Assume that $F$ and $\frac{\partial F}{\partial x_i}$ for $i=1,\dots, n$ are continuous functions of $(x_1,x_2,\dots , x_n)$ on $\mathbb{R}^n$. Then a unique solution exits to the initial value problem
$$\frac{dX}{dt}=F(X), \quad X(t_0)=X_0$$
for any initial value $X_0\in \mathbb{R}^n$.
\end{theorem}

\begin{theorem} (Hartman-Grobman)
Assume that $(\bar x, \bar y)$ is a hyperbolic (all eigenvalues of the Jacobian matrix evaluated at $(\bar x,\bar y)$ have nonzero real part) equilibrium. Then, in a small neighborhood of $(\bar x, \bar y)$, the nonlinear system 
behaves in a similar manner as the linearized system.
\end{theorem}


%Consider the first-order autonomous differential equation
%$$
%\begin{array}{ll}
%\frac{dx}{dt}=& f(x,y),\\
%\frac{dy}{dt}=& g(x,y).
%\end{array}
%$$
\begin{theorem}
Assume the first-order partial derivatives of $f$ and $g$ are continuous in some open set containing the equilibrium $(\bar x,\bar y)$ of the system
$$
\begin{array}{ll}
\frac{dx}{dt}=& f(x,y),\\
\frac{dy}{dt}=& g(x,y).
\end{array}
$$
Then the equilibrium is locally asymptotically stable if
$$\tr J <0 \quad and \quad \det J >0,$$
where $J$ is the Jacobian matrix evaluated at the equilibrium $(\bar x,\bar y)$. In addition, the equilibrium is unstable if either $\tr J >0$ or $\det J<0$.
\end{theorem}

As the linearization is only an approximation of the nonlinear system, the nonlinear system may behave differently from the linear system in 3 cases:
\begin{itemize}
\item $\det (J)=0$: there exists at least one zero eigenvalue, then the equilibrium in the nonlinear system may be a node, a saddle or a spiral.
\item $\tr(J)=0$ and $\det (J)>0$: eigenvalues are purely imaginary. The equilibrium in the nonlinear system may be a spiral or a center.
\item $\tr (J)^2=4\det (J)$: in the nonlinear system the equilibrium may be a node or a spiral.
\end{itemize}


Theorem for $n-$dimensional system:
\begin{theorem}
Suppose $dX/dt=F(X)$ is a nonlinear first-order autonomous system with an equilibrium $\bar X$. Denote the Jacobian matrix of $F$ evaluated at $\bar X$ as $J(\bar X)$. If the characteristic equation of the Jacobian matrix $J(\bar X)$,
$$\lambda ^n + a_1 \lambda ^{n-1}+\dots + a_{n-1}\lambda +a_n=0$$
satisfies the conditions of the Routh-Hurwitz criteria, that is, the determinants of all the Hurwitz matrices are positive $\det (H_j)>0$, $j=1,\dots,n$ then the equilibrium $\bar X$ is L.A.S. If $\det (H_j)<0$ for some $j=1,\dots , n$ then the equilibrium $\bar X$ is unstable.
\end{theorem}


\section{Phase plane analysis}
To study the qualitative behavior of a system without solving it.
$$
\begin{array}{ll}
\frac{dx}{dt}=& f(x,y),\\
\frac{dy}{dt}=& g(x,y)
\end{array}
$$
Solutions curves (trajectories) $(x(t),y(t))$ are parametric equations with $t$ as an parameter.

At any point $(x,y)$, 
$$\frac{dy}{dx}=\frac{g(x,y)}{f(x,y)}$$
is the slope of the trajectory in the $xy-$plane and the tangent vector that gives the direction of the trajectory is $(f(x,y),g(x,y))$. The collection of vectors evaluated at any point of the $xy-$plane defines the direction field.


\begin{definition}
The $x-$nullcline for
$$
\begin{array}{ll}
\frac{dx}{dt}=& f(x,y),\\
\frac{dy}{dt}=& g(x,y)
\end{array}
$$
is the set of all points in the $xy-$plane satisfying $f(x,y)=0$.

The $y-$nullcline for
$$
\begin{array}{ll}
\frac{dx}{dt}=& f(x,y),\\
\frac{dy}{dt}=& g(x,y)
\end{array}
$$
is the set of all points in the $xy-$plane satisfying $g(x,y)=0$.
\end{definition}

At any intersection of $x-$nullcline and $y-$nullcline, there is an equilibrium point.

On the $x-$nullcline, all vectors are vertical. On the $y-$nullcline, all vectors are horizontal.
We need to check if the direction of flow is up or down on the $x-$nullcline, and if the direction of flow is left or right on the $y-$nullcline.

\section{Bifurcations}
Mathematical models have many parameters. When parameter values change, a change in the behavior of the solution can be expected. If the variation of a parameter change the qualitative behavior of the solution, there is a bifurcation.


Consider the differential equation
$$\frac{dx}{dt}=f(x,\mu), \quad x\in \mathbb{R} \quad \mu \in \mathbb{R}.$$
where $\mu$ is the parameter.

\begin{definition}
$\bar x$ is a bifurcation point and $\bar \mu$ is a bifurcation value if
$$f(\bar x, \bar \mu)=0, \quad \textrm{and} \quad \frac{\partial }{\partial x}f(\bar x, \bar \mu)=0.$$
\end{definition}


Different types of bifurcations in the case of scalar differential equations:
\begin{itemize}
\item Saddle-node bifurcation
\item Transcritical bifurcation
\item Pitchfork bifurcation
\end{itemize}
These types of bifurcations also occur in higher-dimensional system.

A fourth type of bifurcation can occurs in systems consisting of two or more equations: the Hopf bifurcation.

Consider a system of autonomous DEs:
\begin{align*}
\frac{dx}{dt}=&f(x,y,r)\\
\frac{dy}{dt}=&g(x,y,r)
\end{align*}
$f$ and $g$ depends on the parameter $r$. Assume that $(\bar x(r), \bar y(r))$ is an equilibrium and the Jacobian evaluated at this equilibrium has eigenvalues $\alpha(r)\pm i \beta (r)$.

A change of stability occurs at $r=\bar r$ where $\alpha (\bar r)=0$. If $\alpha (r)<0$ for $r<\bar r$ and $\alpha (r)>0$ for $r>\bar r$ (with $\beta (r)\not =0$), then the equilibrium changes from a stable spiral to an unstable spiral when $r$ passes through $\bar r$. The Hopf Theorem states that there is a periodic orbit near $r=\bar r$ from any neighborhood of the equilibrium is $\mathbb{R}^2$. Then $r$ is the parameter of bifurcation and the bifurcation value is $\bar r$.


\begin{theorem}(Hopf bifurcation)
Consider the system
\begin{align*}
\frac{dx}{dt}=&f(x,y,r)\\
\frac{dy}{dt}=&g(x,y,r)
\end{align*}
where $f(x,y,r)$ and $g(x,y,r)$ are continous and differentiable. The system has an equilibrium $(\bar x(r), \bar y(r))$, and the Jacobian of the system evaluated at this parameter-dependent equilibrium is $J(r)$. The Jacobian matrix $J(r)$ has eigenvalues $\alpha(r)\pm i \beta (r)$.

Assume that there exists a value $\bar r$ called the bifurcation value, such that $\alpha (\bar r)=0$ and $\beta (\bar r)\not =0$, and as $r$ is varied through, $\bar r$, the real part of the eigenvalues change signs $$\frac{d \alpha}{d r}_{r=\bar r}\not =0.$$

Given these following hypotheses, the following possibilities arise:
\begin{itemize}
\item At $r=\bar r$, a center is created at the equilibrium, and thus infinitely many neutrally stable concentric closed orbits surround $(\bar x(r), \bar y(r))$.
\item There is a range of $r$ values that $\bar r <r<c$ for which a single closed orbit (a limit cycle) surround  $(\bar x(r), \bar y(r))$. As $r$ is varied the diameter of the limit cycle changes in proportion to $\sqrt{|r-\bar r|}$. There are no other closed orbits near  $(\bar x(r), \bar y(r))$. Since the limit cycle exists for values above $\bar r$, this phenomenon is called a supercritical bifurcation.
\item There is a range of values such that $d<r<\bar r$ for which a single closed orbit (a limit cycle) surround  $(\bar x(r), \bar y(r))$. Since the limit cycle exists for values below $\bar r$, this phenomenon is called a supercritical bifurcation.
\end{itemize}
\end{theorem}




%\begin{theorem}(Hopf bifurcation)
%Let $x'=A(\mu)x+F(\mu,x)$ be a $C^k$ planar vector field, with $k\geq 0$, depending on the scalar parameter $\mu$ such that $F(\mu,0)=0$ and $D_xF(\mu,0)=0$ for all $\mu$ sufficiently close enough to the origin. Assume that the linear part $A(\mu)$ at the origin has the eigenvalue $\alpha(\mu)\pm i\beta(\mu)$, with $\alpha(0)=0$ and $\beta(0)\neq 0$. Furthermore, assume the eigenvalues cross the imaginary axis with nonzero speed, i.e.,
%\[
%\left.\frac{d}{d\mu}\alpha(\mu)\right|_{\mu=0}\neq 0.
%\]
%Then, in any neighborhood $\mathcal{U}\ni(0,0)$ in $\mathbb{R}^2$ and any given $\mu_0>0$, there exists a $\bar\mu$ with $|\bar\mu|<\mu_0$ such that the differential equation $x'=A(\bar\mu)x+F(\bar\mu,x)$ has a nontrivial periodic orbit in $\mathcal{U}$.
%\end{theorem}



%Consider
%\[
%\frac{d}{dt}
%\begin{pmatrix}
%x\\ y
%\end{pmatrix}
%=
%\begin{pmatrix}
%\alpha(r) & \beta(r) \\
%-\beta(r) & \alpha(r)
%\end{pmatrix}
%\begin{pmatrix}
%x\\ y
%\end{pmatrix}
%+
%\begin{pmatrix}
%f_1(x,y,r)\\
%g_1(x,y,r)
%\end{pmatrix}
%=\begin{pmatrix}
%f(x,y,r)\\
%g(x,y,r)
%\end{pmatrix}
%\]
%The Jacobian at the origin is
%\[
%J(r)=\begin{pmatrix}
%\alpha(r) & \beta(r) \\
%-\beta(r) & \alpha(r)
%\end{pmatrix}
%\]
%and thus eigenvalues are $\alpha(r)\pm i\beta(r)$, and $\alpha(0)=0$ and $\beta(0)>0$.
%
%
%Define
%\begin{align*}
%C &= f_{xxx}+f_{xyy}+g_{xxy}+g_{yyy} \\
%& \quad+ \frac{1}{\beta(0)}\left(-f_{xy}\left(f_{xx}+f_{yy}\right)+g_{xy}\left(g_{xx}+g_{yy}\right)+f_{xx}g_{xx}-f_{yy}g_{yy}\right),
%\end{align*}
%evaluated at $(0,0)$ and for $r=0$.
%Then, if $d\alpha(0)/dr>0$,
%\begin{enumerate}
%\item If $C<0$, then for $r<0$, the origin is a stable spiral, and for $r>0$, there exists a stable periodic solution and the origin is unstable ({\bf supercritical Hopf}).
%\item If $C>0$, then for $r<0$, there exists an unstable periodic solution and the origin is stable, and for $r>0$, the origin is unstable ({\bf subcritical Hopf}).
%\item If $C=0$, the test is inconclusive.
%\end{enumerate}


%\begin{theorem}(Poincar\'e - Bendixson Theorem)
%If for $t\geq t_0$ a trajectory is bounded and does not approach any equilibrium point, then it is either a closed periodic orbit or approaches a closed periodic orbit for $t \rightarrow +\infty$.
%\end{theorem}

%
%
